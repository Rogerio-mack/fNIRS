{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fnirs_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99b165564b3c4bbf9571786a1c617de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f39e6ec12bd46fc89fbef50c456fa73",
              "IPY_MODEL_e2f97557cec942cda6265d28ad06c1be"
            ],
            "layout": "IPY_MODEL_903a22f9a66a496bb28a9a4c09ae7a4e"
          }
        },
        "9f39e6ec12bd46fc89fbef50c456fa73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c34233762fe47d88cf2bb82c2806b68",
            "max": 17881709,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_007f2a4161ac43818715098745f0c756",
            "value": 17881709
          }
        },
        "e2f97557cec942cda6265d28ad06c1be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fda4eb1c8a34c0db7c883f1c765e112",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_baf25102ac654f6284f1f7bc73e9791e",
            "value": " Downloading : 17.1M/17.1M [00:01&lt;00:00,    10.8MB/s]"
          }
        },
        "903a22f9a66a496bb28a9a4c09ae7a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c34233762fe47d88cf2bb82c2806b68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "007f2a4161ac43818715098745f0c756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4fda4eb1c8a34c0db7c883f1c765e112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baf25102ac654f6284f1f7bc73e9791e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cde927b920904b8ab5904b8b77c6d0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad81f9f2e8aa4b61a43932ccfe6afd00",
              "IPY_MODEL_4348776d41c44de6b379392931474f11"
            ],
            "layout": "IPY_MODEL_696d80e0592e4626a0df418fccb5b059"
          }
        },
        "ad81f9f2e8aa4b61a43932ccfe6afd00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05b235d067ac448a86a39cc8e86bb8e1",
            "max": 1652769680,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c3baec7e8714ee9891adf613536b345",
            "value": 1652769680
          }
        },
        "4348776d41c44de6b379392931474f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aeb1b0b59831423d89489a346a19ebc9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a546faeafae94e5ab807f7c94b8c4bd7",
            "value": " Downloading : 1.54G/1.54G [00:50&lt;00:00,    32.7MB/s]"
          }
        },
        "696d80e0592e4626a0df418fccb5b059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05b235d067ac448a86a39cc8e86bb8e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c3baec7e8714ee9891adf613536b345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "aeb1b0b59831423d89489a346a19ebc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a546faeafae94e5ab807f7c94b8c4bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rogerio-mack/fNIRS/blob/main/fnirs_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XXMbfcRa8pZ"
      },
      "source": [
        "# **fNIRS**, uma IntroduÃ§Ã£o: PrincÃ­pios, Dispositivos, Tratamento e ClassificaÃ§Ã£o dos Sinais \n",
        "\n",
        "Autor: *rogerio.oliveira@mackenzie.br*\n",
        "\n",
        "FiliaÃ§Ã£o: Universidade Presbiteriana Mackenzie\n",
        "\n",
        "Data: 2021-09-20\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRQGfjpOcNlP"
      },
      "source": [
        "## Abstract\n",
        "\n",
        "Esta revisÃ£o Ã© uma introduÃ§Ã£o Ã  espectroscopia funcional de infravermelho prÃ³ximo (fNIRS) para seu uso prÃ¡tico, em especial em neurociÃªncias. Ela aborda desde os princÃ­pios gerais da espectroscopia de infravermelho prÃ³ximo atÃ© a construÃ§Ã£o de modelos simples de mapeamento funcional. Fornece assim, uma visÃ£o geral do uso de fNIRS para os interessados no processamento de imagens e sinais cerebrais aplicados Ã  neurociÃªncia como tambÃ©m um primeiro guia prÃ¡tico para experimentaÃ§Ã£o e uso de softwares livres de anÃ¡lise. Ao final indicamos algumas das potenciais vantagens deste roteiro para o uso de tÃ©cnicas de anÃ¡lise dos sinais fNIRS ainda pouco explorados.\n",
        "\n",
        "**Palavras-Chave**: *fNIRS, espectroscopia funcional de infravermelho prÃ³ximo, neurociÃªncias, neuroimagem, deep learning*  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3ATZFWQgCdk"
      },
      "source": [
        "## IntroduÃ§Ã£o\n",
        "\n",
        "Este estudo fornece aos interessados em neurociÃªncias, e em particular aos interessados no uso prÃ¡tico de neuroimagens, uma visÃ£o geral da espectroscopia funcional de infravermelho prÃ³ximo (fNIRS) para o desenvolvimento de experimentos e anÃ¡lise de imagens cerebrais.\n",
        "\n",
        "Desde seu surgimento na dÃ©cada de 80 espectroscopia funcional de infravermelho prÃ³ximo (fNIRS) vem se mostrando como uma alternativa viÃ¡vel, nÃ£o invasiva, versÃ¡til e de baixo custo (*) quando comparados a outras tÃ©cnicas como a tomografia por emissÃ£o positrons (PET e SPECT), magnetoencefalografia (MEG) e ressonÃ¢ncia magnÃ©tica funcional (fMRI) [cite].  Assim como as tÃ©cnicas de PET, SPECT e fMRI, o fNRIS Ã© um mÃ©todo indireto que mede a atividade cerebral a partir das alteraÃ§Ãµes da homodinÃ¢mica cerebral no que se diferencia de outros mÃ©todos que medem diretamente a atividade elÃ©trica do cÃ©rebro como o MEG e EEG (eletroencelografia) [cite]. A fNIRS vem desempenhando um papel importante no mapeamento das funÃ§Ãµes do cerÃ©bro [cite], e seu baixo custo e mobilidade abrem espaÃ§o para muitas novas aplicaÃ§Ãµes clÃ­nicas [cite] e de interface homem-mÃ¡quina [cite].\n",
        "\n",
        "O uso prÃ¡tico da fNIRS requer, entretanto, uma espectro amplo de conhecimentos que vÃ£o desde o sistema de fNIRS empregado e o desenho correto do experimento para a captura dos dados, atÃ© o emprego de vÃ¡rias tÃ©cnicas para o tratamento e anÃ¡lise do sinais [cite]. Em conjunto com tÃ©cnicas estatÃ­sticas mais tradicionais, como modelo lineares generalizados, modelos de aprendizado de mÃ¡quina vem sendo empregados para o tratamento e classificaÃ§Ã£o dos sinais e,mais recentemente, modelos de redes neurais profundas parecem permitir o emprego de direto de dados brutos de fNIRS [cite]. \n",
        "\n",
        "Dadas as diferentes aplicaÃ§Ãµes que se pode dar a fNIRS, em especial as que envolvem mobilidade e aplicaÃ§Ãµes interface homem-mÃ¡quina, e variedade de tÃ©cnicas e usos potenciais dos sinais de fNIRS para essas aplicaÃ§Ãµes, Ã© importante que as ferramentas para uso dessas tÃ©cnicas sejam o mais possÃ­vel abertas e acessÃ­veis, permitindo maior flexibilidade de pesquisa e construÃ§Ã£o de aplicaÃ§Ãµes e novos mÃ©todos. Nesse sentido, o uso de soluÃ§Ãµes de cÃ³digo aberto (como tambÃ©m de hardware) parece ser a alternativa mais adequada. Nessa perpectiva este estudo contribui apresentando um roteiro prÃ¡tico, da aquisiÃ§Ã£o atÃ© o processamento dos sinais fNIRS, empregando soluÃ§Ãµes de cÃ³digo aberto para pesquisadores e interessados de quaisquer Ã¡reas. \n",
        "\n",
        "O estudo encontra-se organizado como segue. A primeira seÃ§Ã£o descreve brevemente as bases biolÃ³gicas e fÃ­sicas da espectroscopia funcional de infravermelho prÃ³ximo (fNIRS). Em seguida sÃ£o descritos os diferentes sistemas de fNIRS, seus tipos de iluminaÃ§Ã£o e coleta de dados. A seÃ§Ã£o trÃªs apresenta algumas aplicaÃ§Ãµes dessa tÃ©ncnica e discute vantagens e limitaÃ§Ãµes do seu uso. As seÃ§Ãµes seguintes apresentam um roteiro prÃ¡tico de anÃ¡lise dos dados de fNIRS, desde a aquisiÃ§Ã£o e o tratamento dos sinais, atÃ© o emprego de tÃ©cnicas de aprendizado de mÃ¡quina para a classificaÃ§Ã£o dos sinais com o emprego softwares livres. Ao final indicamos vantagens do roteiro proposto indicando possÃ­veis tÃ©nicas de anÃ¡lise ainda pouco exploradas.\n",
        "\n",
        "De modo a proporcionar fÃ¡cil acesso ao uso desse tutorial todo o cÃ³digo e dados encontram-se disponÃ­veis publicamente no `https://github.com/Rogerio-mack` e, embora o estudo esteja focado em aplicaÃ§Ãµes de neurociÃªncia boa parte dos temas aqui tratados podem ser igualmente empregados em outras aplicaÃ§Ãµes da espectroscopia de infravermelho prÃ³ximo, como outras aplicaÃ§Ãµes em saÃºde (oxigenaÃ§Ã£o de mÃºsculos e outros Ã³rgÃ£os) e engenharia de alimentos. \n",
        "\n",
        "<small> Baixo custo, [2] aponta a equipamentos que vÃ£o de USD 10.000 a USD 100.000. Uma pesquisa na internet indica haverem equipamentos comerciais mÃ³veis, prÃ©-frontais, de atÃ© 8 emissores a partir USD 5.000, e um sistema anÃ¡logo parece poder ser construÃ­do de modo aberto com cerca de USD 2.000 (~ USD 250 por canal [HW1]).\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M80sf4XGLEyT"
      },
      "source": [
        "## PrincÃ­pios de fNIRS\n",
        "\n",
        "AlÃ©m dos neurÃ´nios e cÃ©lulas gliais, o suprimento de sangue no cÃ©rebro desempenha papel essencial no funcionamento do cÃ©rebro. Os neurÃ´nios nÃ£o tÃªm nenhuma provisÃ£o substancial de oxigÃªnio ou glicose, e aumentos na atividade neural requerem um aumento no suprimento de sangue. Essa relaÃ§Ã£o entre a atividade neural e a hemodinÃ¢mica cerebral Ã© denominada de acoplamento neurovascular. Embora nÃ£o seja totalmente compreendido, o acoplamento neurovascular fornece um sinal indireto da atividade cerebral e Ã© o princÃ­pio de sistemas de neuroimagem como o fMRI e fNIRS. Essa dependÃªncia comum pelo princÃ­pio do acoplamento neurovascular permite que as duas tÃ©cnicas compartilhem vÃ¡rias aplicaÃ§Ãµes e mÃ©todos de anÃ¡lise.\n",
        "\n",
        "Enquanto os sinais fMRI (BOLD ou Blood-oxygen-level-dependent imaging) medem mudanÃ§as na suscetibilidade magnÃ©tica do sangue fornecendo uma medida da atividade cerebral, o fNIRS explora as propriedades Ã³pticas dos tecidos, principalmente do sangue, medindo a absorÃ§Ã£o da luz na faixa NIR para inferir sobre a atividade cerebral. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeX90ANn3TGZ"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/Rogerio-mack/fNIRS/main/images/Fig_Blood_Oxi_Coupling.png\" width=800, align=\"center\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuT5DA77q_Fp"
      },
      "source": [
        "<small> <b> Fig. 1. EsquemÃ¡tico de um vaso capilar do tecido cerebral durante o repouso (esquerda) e durante a atividade neural (direita). Os cÃ­rculos vermelhos e azuis representam respectivamente os glÃ³bulos vermelhos totalmente oxigenados (oxi-hemoglobina, HbO2) e totalmente desoxigenados (Hb, desoxi-hemoglobina) (fonte: Glover [4])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZJlNcv_f7E6"
      },
      "source": [
        "Em princÃ­pio os sinais para neuroimagem de fNIRS seguem o mesmo princÃ­pio de um oximetro comum. O sinal de luz atravessa os tecidos e sinal resultante Ã© medido por um foto diodo. A diferenÃ§a do sinal emitido e resultante depende de caracterÃ­sticas fÃ­sicas do meio em transmitir, absorver, refletir e dissipar os sinais e pode fornecer informaÃ§Ãµes sobre as propriedades do tecido. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5r0bUEq5lHx"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_LED_and_Photodiodo.png?raw=true\" width=800, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKM2MxzAffh6"
      },
      "source": [
        "<small> <b> Fig. 2. Em princÃ­pio os sinais para neuroimagem de fNIRS seguem o mesmo princÃ­pio de um oximetro comum (imagens adaptadas de: https://protosupplies.com/product/heartbeat-sensor-module/ e https://www.roswellpark.org/cancertalk/202004/should-you-buy-pulse-oximeter-covid-19-question). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWYF0FU5XJHN"
      },
      "source": [
        "### Propriedades Ã³pticas\n",
        "\n",
        "O fNIRS emprega luz infravermelha prÃ³xima de comprimentos de onda maiores do que o visÃ­vel (750 nm - 1200 nm). A propagaÃ§Ã£o da luz depende do comprimento de onda e de muitas propriedades Ã³pticas e fÃ­sicas do meio (constituiÃ§Ã£o quÃ­mica e consistÃªncia) e do sinal emitido (Ã¢ngulo de incidÃªncia) que determinam a transmissÃ£o, absorÃ§Ã£o, reflexÃ£o e dissipaÃ§Ã£o da onda. \n",
        "\n",
        "Os grupos que absorvem luz em comprimentos de onda especÃ­ficos sÃ£o a hemoglobina oxigenada HbO$_2$, a hemoglobina desoxigenada Hb e sua soma, hemoglobina total HbT, alÃ©m do Citocromo c oxidase (Caa$_3$), e suas concentraÃ§Ãµes mudam com o tempo e com a concentraÃ§Ã£o de oxigÃªnio. A faixa de luz  de 750 nm - 1200 nm fornece uma *janela Ã³ptica* interessante para os sinais de fNIRS pois apresenta uma absorÃ§Ã£o predominante do grupo da hemoglobina (oxigenada, desoxigenada e total). \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2iPAfOc56cD"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_Light_Waves_Absortion.png?raw=true\" width=500, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfD5c9HEsw44"
      },
      "source": [
        "<small> <b> Fig. 3. Espectro de absorÃ§Ã£o de Hb, HbO2, H2O e outros cromÃ³foros na faixa NIR. O ponto isosbÃ©stico do espectro de absorÃ§Ã£o de HB / HbO 2 Ã© circulado em branco dentro da janela Ã³ptica do NIR. (fonte: [1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO7byOB4etPs"
      },
      "source": [
        "A escolha dos comprimentos de onda Ã³timos, dois ou mais, a serem empregados Ã© uma das etapas de desenho do experimento [R2]. Um uso comum Ã© o emprego de dois comprimentos, um entre 650-730nm e outro ~830nm. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQouec6PvBUY"
      },
      "source": [
        "## Determinando HbO2 e Hb\n",
        "\n",
        "As concentraÃ§Ãµes de oxi-hemoglobina, desoxi-hemoglobina e total nÃ£o podem ser medidas diretamente, mas podem ser calculadas a partir dos dados brutos das quantidades de intensidade de luz  medidas no experimento. A absorÃ§Ã£o de luz Ã© medida pelo coeficiente de extinÃ§Ã£o molar $A$ em funÃ§Ã£o do comprimento de onda e existem vÃ¡rias formas para se converter esses dados brutos nas concentraÃ§Ãµes de oxi-hemoglobina, desoxi-hemoglobina e total. A forma mais comum em fNIRS Ã© o emprego da Lei Modificada de Beer-Lambert (MBLL, Modified Beer-Lambert Law) serÃ¡ o procedimento empregado aqui mais adiante. A MBLL estabelece que a mudanÃ§a na atenuaÃ§Ã£o da luz Ã© proporcional Ã s mudanÃ§as nas concentraÃ§Ãµes dos cromÃ³foros do tecido, principalmente oxi e desoxi-hemoglobina e permite calcular suas concentraÃ§Ãµes a partir das mudanÃ§as de atenuaÃ§Ã£o medidas em dois ou mais comprimentos de onda assumindo um meio homogÃªneo em que a absorÃ§Ã£o do  tecido muda de maneira homogÃªnea e a difusÃ£o Ã© constante [R5]. Essas premissas nÃ£o sÃ£o totalmente verdadeiras para o tecido cerebral, mas essa abordagem Ã© bastante empregada nÃ£o parece afetar significativamente os resultados qualitativos [R2]. \n",
        "\n",
        "### Lei Modificada de Beer-Lambert \n",
        "\n",
        "A lei de Beer-Lambert relaciona a atenuaÃ§Ã£o da luz por absorÃ§Ã£o Ã  concentraÃ§Ã£o de cromÃ³foro [R5]:\n",
        "\n",
        "\\begin{align*} A = ln( \\frac{I_i}{I_d} ) = L \\mu_a +G, \\end{align*}\n",
        "\n",
        "onde $A$ Ã© a atenuaÃ§Ã£o do sinal de luz, $I_i$ e $I_d$ sÃ£o respectivamente intensidade de luz incidente e a detectada, $\\mu_a$ Ã© o coeficiente de absorÃ§Ã£o do cromÃ³foro e $L$ a distÃ¢ncia mÃ©dia entre a fonte de luz e o detector. $G$ Ã© um fator dependente da geometria dos optodos e envolve a perda de intensidade causada pelo espalhamento. Se considerarmos que a absorÃ§Ã£o muda homogeneamente e $G$ constante, podemos eliminar $G$ e obter as diferenÃ§as de atenuaÃ§Ã£o como : \n",
        "\n",
        "\\begin{align*} \\Delta A = ln( \\frac{I_d^1}{I_d^2} ) = L \\Delta \\mu_a, \\end{align*}\n",
        "\n",
        "Assim, a diferenÃ§a de atenuaÃ§Ã£o Ã© calculada a partir da intensidade de dois\n",
        "diferentes estados do tecido, e Ã© proporcional Ã  diferenÃ§a de absorÃ§Ã£o que ainda pode ser reescrita com base nas concentraÃ§Ã£o dos cromÃ³foros de oxi e\n",
        "desoxi-hemoglobina:\n",
        "\n",
        "\\begin{align*} \\Delta \\mu_a = a_{HbO_2}. \\Delta C_{HbO_2} + a_{Hb}. \\Delta C_{Hb} \\end{align*}\n",
        "\n",
        "E cÃ¡lculo das variaÃ§Ãµes de concentraÃ§Ã£o de oxi-hemoglobina, desoxi-hemoglobina para dois comprimentos de onda, $\\lambda_1$ e $\\lambda_2$, pela MBLL Ã© dado entÃ£o por:\n",
        "\n",
        "\\begin{align*} \\Delta C_{HbO_2} &= \\frac{a_{Hb}^{\\lambda _1}. \\Delta A_{\\lambda _2} / L_{\\lambda _2} - a_{Hb}^{\\lambda _2}. \\Delta A_{\\lambda _1} / L_{\\lambda _1}}{(a_{HbO_2}^{\\lambda _1}. a_{Hb}^{\\lambda _2} - a_{HbO_2}^{\\lambda _2}. a_{Hb}^{\\lambda _1}) }\n",
        "\\\\ \\Delta C_{Hb} &= \\frac{a_{HbO_2}^{\\lambda _1}. \\Delta A_{\\lambda _2} / L_{\\lambda _2} - a_{HbO_2}^{\\lambda _2}. \\Delta A_{\\lambda _1} / L_{\\lambda_1}} {(a_{HbO_2}^{\\lambda _1}. {a_{Hb}^{\\lambda _2} - a_{HbO_2}^{\\lambda _2}. a_{Hb}^{\\lambda _1}})} \\end{align*}\n",
        "\n",
        "Para mais de dois comprimentos de onda o cÃ¡lculo requer o uso do mÃ©todo de mÃ­nimos quadrados ou tÃ©cnicas semelhantes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmXdxUmL6Ucm"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_General_fNIRS.png?raw=true\" width=1100, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A2pf9SN0yIm"
      },
      "source": [
        "<small> <b> Fig. 4. EstÃ¡gios de aquisiÃ§Ã£o de dados fNIRS. (a) A luz Ã© projetada ao longo de um caminho em forma de banana do optodo de luz atravÃ©s do couro cabeludo, crÃ¢nio, lÃ­quido cefalorraquidiano e no cÃ³rtex. A luz Ã© absorvida,\n",
        "espalhados e refletidos da cabeÃ§a para o detector. (b) MudanÃ§as na intensidade da luz estÃ£o relacionadas Ã s mudanÃ§as de concentraÃ§Ã£o em\n",
        "hemoglobina pela lei modificada de Beer-Lambert. Isso produz a funÃ§Ã£o de resposta hemodinÃ¢mica (HRF). (c) Oxi-Hb\n",
        "Ã© representado pela linha vermelha, enquanto a desoxi-Hb Ã© representada pela linha azul. O eixo x Ã© o tempo em segundos, e o\n",
        "eixo y Ã© a mudanÃ§a relativa da concentraÃ§Ã£o dos cromÃ³foros (adaptado de [T1])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdiRoE9rLika"
      },
      "source": [
        "## Diferentes Sistemas de fNIRS\n",
        "\n",
        "De modo simples e de um ponto de vista tÃ©cnico um sistema fNIRS pode ser entendido como uma fonte de luz NIR, acoplada e emitida no couro cabeludo, e um detector que mede a refletÃ¢ncia difusa do sinal que reemerge do tecido a\n",
        "a poucos centÃ­metros da fonte. De qualquer modo os sistemas de fNIRS podem diferir em muitos aspectos como, a forma como sÃ£o medidos os sinais de luz, tipo e quantidade de fontes empregados, comprimentos de onda e os tipos de detectores, e essas caracterÃ­sticas tem forte impacto nas limitaÃ§Ãµes e no alcance do experimento que se quer realizar.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mpo-SUdCfpt"
      },
      "source": [
        "##  Medidas do Sinal de Luz\n",
        "\n",
        "Existem trÃªs tipos principais de sistemas NIRS: \n",
        "\n",
        "1. Sistemas de Onda ContÃ­nua\n",
        "1. EspectrÃ´metros de DomÃ­nio do Tempo \n",
        "1. EspectrÃ´metros de DomÃ­nio da FreqÃ¼Ãªncia\n",
        "\n",
        "Os sistemas de Onda ContÃ­nua sÃ£o os mais antigos e mais comumente empregado comercialmente. Eles emitem luz em uma intensidade constante e entÃ£o medem  apenas as mudanÃ§as na intensidade da luz refletida difusamente atravÃ©s do tecido com um fotodetector ou um fotodiodo, e empregam  dois ou mais comprimentos de onda emitidos por mÃºltiplas fontes. Esses sistemas, entretanto, nÃ£o conseguem separar a atenuaÃ§Ã£o da absorÃ§Ã£o e do espalhamento e, portanto, nÃ£o podemos obter as concentraÃ§Ãµes de oxi e desoxi-hemoglobina de forma absoluta e empregam a diferenÃ§a relativa das concentraÃ§Ãµes.  Eles tambÃ©m pode apresentar baixa profundidade de penetraÃ§Ã£o da luz devido Ã  curta distÃ¢ncia fonte-detector. Apesar disso, esses sistemas tem vantagens em simplicidade, tamanho, peso e custo quando comparados aos demais sistemas de domÃ­nio do tempo e da frequÃªncia, que medem nÃ£o sÃ³ a intensidade da luz, mas tambÃ©m o tempo que a luz leva para viajar atravÃ©s do tecido. Para essas medidas os sistemas sÃ£o bastante mais complexos e podem envolver fontes de laser e medidores sensÃ­veis de fotÃ³n unico. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il5Q6tJN6w9U"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_Signal_Types.png?raw=true\" width=800, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oheD72ICvDO5"
      },
      "source": [
        "<small> <b> Fig 5. Diferentes tÃ©cnicas de medida do sinal (adaptado de [R2])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ9EgJ9L7mQR"
      },
      "source": [
        "## Emissores e Detectores\n",
        "\n",
        "Na maioria dos sistemas a fonte de luz Ã© um diodo emissor de luz (LED) [R1, R2]. O LED Ã© um diodo semicondutor que quando Ã© energizado emite luz visÃ­vel. A luz nÃ£o Ã© monocromÃ¡tica (como em um laser), mas consiste de uma banda espectral relativamente estreita e Ã© produzida pelas interaÃ§Ãµes energÃ©ticas do elÃ©tron. Outros sistemas empregam ainda diodos laser, mas os LEDs sÃ£o frequentemente preferidos por razÃµes de seguranÃ§a [R1] (por exemplo, devido a problemas de aquecimento por uma exposiÃ§Ã£o prolongada). Em quaisquer casos as faixas de onda  ficam entre 685-780 nm e 805-850 nm. \n",
        "\n",
        "Os detectores mais empregados sÃ£o fotodiodos. Um fotodiodo Ã© um dispositivo de junÃ§Ã£o pn semicondutor que converte luz em corrente elÃ©trica. HÃ¡ ainda sistemas que empregam fotodiodos de avalanche  e tubos fotomultiplicadores [R1, R2]. Em quaisquer casos uma preocupaÃ§Ã£o no projeto e operaÃ§Ã£o dos detectores Ã© a necessidade de bloquear a luz ambiente uma vez que eles apresentam baixa seletividade de sinal [R1]. \n",
        "\n",
        "Tanto para os emissores como para os detectores uma preocupaÃ§Ã£o importante no projeto e operaÃ§Ã£o dos optodos Ã© na interferÃªncia do cabelo para emissÃ£o e detecÃ§Ã£o dos sinais de luz sendo importante tÃ©cnicas para melhor contato e ajuste dos optodos, como a capacidade de lidar com diferentes quantidades de cabelo, diferentes texturas de cabelo e diferentes cores para experimentos e sistemas inclusivos (https://research.fb.com/programs/research-awards/proposals/request-for-proposals-on-engineering-approaches-to-responsible-neural-interface-design/). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n63NobiiJ8oi"
      },
      "source": [
        "## Coleta dos dados \n",
        "\n",
        "A maior parte dos dispositivos fornece intensidades de luz brutas. Mas existem ainda poucos sistemas que fornecem densidades Ã³pticas ou dados diretos das concentraÃ§Ãµes de oxi e desoxi-homoglobina o que nÃ£o Ã© adequado para o emprego de algoritmos customizados [R2]. \n",
        "\n",
        "Os dados em geral sÃ£o amplificados e armazenados no mesmo dispositivo ou ainda transmitidos para um outro dispositivo para o tratamento dos dados (reduÃ§Ã£o de ruÃ­do, transformaÃ§Ãµes, anÃ¡lise). As soluÃ§Ãµes abertas em geral empregam um computador ou um celular com uso de software aberto ou scripts customizados para o tratamento dos dados, e aqui nossa opÃ§Ã£o Ã© pelo uso de scripts em Ptyhon.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnDs2WUHNbiA"
      },
      "source": [
        "## Mobilidade\n",
        "\n",
        "Sistemas fNIRS podem ter a luz NIR direcionada diretamente para o couro cabeludo a partir da fonte NIR ou transmitida por fibras Ã³pticas a partir de um dispositivo central. Do mesmo modo a luz NIR refletida Ã© recebida da cabeÃ§a diretamente pelo detector NIR ou guiada por meio de fibras Ã³pticas para um detector NIR central. Os sistemas do primeiro modo (direto) sÃ£o sistemas vestÃ­veis que permitem mobilidade do sujeito. SÃ£o sistemas de menor capacidade mas que viabilizam uma sÃ©rie de experimentos que requerem a mobilidade do sujeito. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdzP3iGBvryb"
      },
      "source": [
        "## Arranjo dos Emissores e Detectores\n",
        "\n",
        "Um sensor fNIR possui dois componentes: um emissor e um detector. Cada par  forma um canal, um caminho entre uma fonte de luz e um detector. Por exemplo podemos ter um emissor e dois detectores para criar dois canais. Em geral o nÃºmero de detectores Ã© maior que o nÃºmero de emissores e, na prÃ¡tica, o nÃºmero de canais Ã© bastante menor que o nÃºmero de pares emissor-detector, uma vez que os canais utilizÃ¡veis nÃ£o podem ter uma distÃ¢ncia muito grande entre a fonte e o detector de luz o que tambÃ©m irÃ¡ depender do arranjo dos optodos no crÃ¢nio.\n",
        "\n",
        "De qualquer maneira a disposiÃ§Ã£o dos emissores e detectores em geral seguem um de 3 tipos de esquemas:\n",
        "\n",
        "1. TransiluminaÃ§Ã£o\n",
        "1. RefletÃ¢ncia\n",
        "1. RefletÃ¢ncia diferencial\n",
        "\n",
        "O esquema de transiluminaÃ§Ã£o, pela distÃ¢ncia do emissor-detector, sÃ³ pode ser empregado para recÃ©m-nascidos, embora a tÃ©cnica possa ser empregada para outros fins em outros orgÃ£os do corpo humano. O modo de refletÃ¢ncia Ã© o mais utilizado nos dispositivos de fNIRS. A regiÃ£o sensÃ­vel entre emissor-detector assume uma forma de *banana* fazendo com que a profundidade de penetraÃ§Ã£o do NIRS dependa da distÃ¢ncia do emissor-detector, sendo estimada em cerca de 1/3 dessa distÃ¢ncia [R1]. Tipicamente empregam-se distÃ¢ncias de 3-4 cm permitindo uma profundidade de 1-2 cm a depender da posiÃ§Ã£o do optodo na cabeÃ§a e do sujeito [R1, R3] e muitos sistemas podem ter a distÃ¢ncia fixa (nÃ£o ajustÃ¡vel) no suporte que envolve os optodos. Na refletÃ¢ncia diferencial mais um detector ou fonte sÃ£o empregados para medir a diferenÃ§a entre os caminhos de luz.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AznNo5NO7PwT"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_Emissor_Detector_Schemas.png?raw=true\" width=600, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-PwfSL23QkF"
      },
      "source": [
        "<small> <b> Fig. 6. Diferentes modos de operaÃ§Ã£o NIRS: transiluminÃ¢ncia, refletÃ¢ncia e refletÃ¢ncia diferencial (adaptado de [R1])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPyNo_qQ7n4l"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_Tissue_Absortion.png?raw=true\" width=800, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpn2nRZA766R"
      },
      "source": [
        "<small> <b> Fig. 7. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubbw0oK98B8t"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_Machine_Learning.png?raw=true\" width=800, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6MVK1nA8B8u"
      },
      "source": [
        "<small> <b> Fig. 8. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMmFm5hMLoLy"
      },
      "source": [
        "## Processamento dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR-fEELU7bwW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0zFa0MAL3A8"
      },
      "source": [
        "## Aprendizado de MÃ¡quina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9PI1QBKL7RK"
      },
      "source": [
        "## Aprendizado Profundo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4WoYmQpMAqa"
      },
      "source": [
        "## ConclusÃµes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wayAfhz8oSTp"
      },
      "source": [
        "## ReferÃªncias\n",
        "\n",
        "### RevisÃ£o\n",
        "\n",
        "1. R. K. Almajidy, K. Mankodiya, M. Abtahi and U. G. Hofmann, \"A Newcomer's Guide to Functional Near Infrared Spectroscopy Experiments,\" in IEEE Reviews in Biomedical Engineering, vol. 13, pp. 292-308, 2020, doi: 10.1109/RBME.2019.2944351.\n",
        "\n",
        "1. Scholkmann, F., Kleiser, S., Metz, A. J., Zimmermann, R., Mata Pavia, J., Wolf, U., & Wolf, M. (2014). A review on continuous wave functional near-infrared spectroscopy and imaging instrumentation and methodology. NeuroImage, 85, 6â€“27. doi:10.1016/j.neuroimage.2013.05.004 \n",
        "\n",
        "1. Strangman, G., Boas, D. A., & Sutton, J. P. (2002). Non-invasive neuroimaging using near-infrared light. Biological Psychiatry, 52(7), 679â€“693. doi:10.1016/s0006-3223(02)01550-0 \n",
        "\n",
        "1. Glover G. H. (2011). Overview of functional magnetic resonance imaging. Neurosurgery clinics of North America, 22(2), 133â€“vii. https://doi.org/10.1016/j.nec.2010.11.001\n",
        "\n",
        "1. Kocsis L, Herman P, Eke A. The modified Beer-Lambert law revisited. Phys Med Biol. 2006 Mar 7;51(5):N91-8. doi: 10.1088/0031-9155/51/5/N02. Epub 2006 Feb 15. PMID: 16481677.\n",
        "\n",
        "1. S. Lloyd-Fox, A. Blasi, C.E. Elwell, Illuminating the developing brain: The past, present and future of functional near infrared spectroscopy,\n",
        "Neuroscience & Biobehavioral Reviews, Volume 34, Issue 3, 2010, Pages 269-284, ISSN 0149-7634, https://doi.org/10.1016/j.neubiorev.2009.07.008.\n",
        "\n",
        "## Hardware\n",
        "\n",
        "1. Francis Tsow, Anupam Kumar, SM Hadi Hosseini, Audrey Bowden,\n",
        "A low-cost, wearable, do-it-yourself functional near-infrared spectroscopy (DIY-fNIRS) headband, HardwareX, Volume 10, 2021, e00204, ISSN 2468-0672,\n",
        "https://doi.org/10.1016/j.ohx.2021.e00204. \n",
        "\n",
        "## AplicaÃ§Ãµes\n",
        "\n",
        "1. Bakker, A., Smith, B., Ainslie, P., & Smith, K. (2012). Near-Infrared Spectroscopy. Applied Aspects of Ultrasonography in Humans. doi:10.5772/32493 \n",
        "\n",
        "1. Berkman, E. T., & Falk, E. B. (2013). Beyond Brain Mapping. Current Directions in Psychological Science, 22(1), 45â€“50. doi:10.1177/0963721412469394 \n",
        "\n",
        "1. Di Domenico, S. I., Rodrigo, A. H., Dong, M., Fournier, M. A., Ayaz, H., Ryan, R. M., & Ruocco, A. C. Functional Near-Infrared Spectroscopy:\n",
        "Proof of Concept for Its Application in Social Neuroscience (2019). Neuroergonomics, 169â€“173. doi:10.1016/b978-0-12-811926-6.00028-2 \n",
        "\n",
        "1. Irani, F., Platek, S. M., Bunce, S., Ruocco, A. C., & Chute, D. (2007). Functional Near Infrared Spectroscopy (fNIRS): An Emerging Neuroimaging Technology with Important Applications for the Study of Brain Disorders. The Clinical Neuropsychologist, 21(1), 9â€“37. doi:10.1080/13854040600910018 \n",
        "\n",
        "## Interface Homem-MÃ¡quina\n",
        "\n",
        "1. Noman Naseer 1 and Keum-Shik Hong, fNIRS-based brain-computer interfaces: a review, Front. Hum. Neurosci., 28 January 2015, https://doi.org/10.3389/fnhum.2015.00003. \n",
        "\n",
        "1. Moses, D.A., Leonard, M.K., Makin, J.G. et al. Real-time decoding of question-and-answer speech dialogue using human cortical activity. Nat Commun 10, 3096 (2019). https://doi.org/10.1038/s41467-019-10994-4\n",
        "\n",
        "1. FaceBook. Request for proposals on engineering approaches to responsible neural interface design. DisponÃ­vel em: https://research.fb.com/programs/research-awards/proposals/request-for-proposals-on-engineering-approaches-to-responsible-neural-interface-design/. Acesso: 16/07/2021.\n",
        "\n",
        "1. FaceBook. BCI milestone: New research from UCSF with support from Facebook shows the potential of brain-computer interfaces for restoring speech communication. DisponÃ­vel em: https://tech.fb.com/bci-milestone-new-research-from-ucsf-with-support-from-facebook-shows-the-potential-of-brain-computer-interfaces-for-restoring-speech-communication/. Acesso: 16/07/2021.\n",
        "\n",
        "## Tratamento e AnÃ¡lise dos Sinais\n",
        "\n",
        "1. Dans, P.W.; Foglia, S.D.; Nelson, A.J. Data Processing in\n",
        "Functional Near-Infrared Spectroscopy (fNIRS) Motor Control\n",
        "Research. Brain Sci. 2021, 11, 606. https://doi.org/10.3390/ \n",
        "\n",
        "1. Herold F, Wiegel P, Scholkmann F, MÃ¼ller NG. Applications of Functional Near-Infrared Spectroscopy (fNIRS) Neuroimaging in Exerciseâ€“Cognition Science: A Systematic, Methodology-Focused Review. Journal of Clinical Medicine. 2018; 7(12):466. https://doi.org/10.3390/jcm7120466\n",
        "\n",
        "1. Worsley, K. J., Liao, C. H., Aston, J., Petre, V., Duncan, G. H., Morales, F., & Evans, A. C. (2002). A General Statistical Analysis for fMRI Data. NeuroImage, 15(1), 1â€“15. doi:10.1006/nimg.2001.0933  \n",
        "\n",
        "1. Takahiro ImaiTakanori SatoIsao NambuYasuhiro Wada, Estimating Brain Activity of Motor Learning by Using fNIRS-GLM Analysis, Neural Information Processing, 2012, Springer Berlin Heidelberg, 401--408, 10.1007/978-3-642-34475-6_48 \n",
        "\n",
        "1. Meryem A. YÃ¼cel, Alexander v. LÃ¼hmann, Felix Scholkmann, Judit Gervain, Ippeita Dan, Hasan Ayaz, David Boas, Robert J. Cooper, Joseph Culver, Clare E. Elwell, Adam Eggebrecht, Maria A. Franceschini, Christophe Grova, Fumitaka Homae, FrÃ©dÃ©ric Lesage, Hellmuth Obrig, Ilias Tachtsidis, Sungho Tak, Yunjie Tong, Alessandro Torricelli, Heidrun Wabnitz, Martin Wolf, \"Best practices for fNIRS publications,\" Neurophoton. 8(1) 012101 (7 January 2021) https://doi.org/10.1117/1.NPh.8.1.012101\n",
        "\n",
        "1. von LÃ¼hmann A, Ortega-Martinez A, Boas DA and YÃ¼cel MA (2020) Using the General Linear Model to Improve Performance in fNIRS Single Trial Analysis and Classification: A Perspective. Front. Hum. Neurosci. 14:30. doi: 10.3389/fnhum.2020.00030\n",
        "\n",
        "1. Worsley KJ, Liao CH, Aston J, Petre V, Duncan GH, Morales F, Evans AC. A general statistical analysis for fMRI data. Neuroimage. 2002 Jan;15(1):1-15. doi: 10.1006/nimg.2001.0933. PMID: 11771969.\n",
        "\n",
        "1. Friston KJ, Holmes AP, Poline JB, Grasby PJ, Williams SC, Frackowiak RS, Turner R. Analysis of fMRI time-series revisited. Neuroimage. 1995 Mar;2(1):45-53. doi: 10.1006/nimg.1995.1007. PMID: 9343589.\n",
        "\n",
        "## Aprendizado de MÃ¡quina e Aprendizado Profundo\n",
        "\n",
        "1. Fernandez Rojas, R., Huang, X. & Ou, KL. A Machine Learning Approach for the Identification of a Biomarker of Human Pain using fNIRS. Sci Rep 9, 5645 (2019). https://doi.org/10.1038/s41598-019-42098-w\n",
        "\n",
        "1. Gao, Yuanyuan & Chao, Hanqing & Cavuoto, Lora & Yan, Pingkun & Kruger, Uwe & Norfleet, Jack & Makled, Basiel & Schwaitzberg, Steven & De, Suvranu & Intes, Xavier. (2020). Deep Learning-based Motion Artifact Removal in Functional Near-Infrared Spectroscopy (fNIRS). 10.13140/RG.2.2.33766.86089. \n",
        "\n",
        "1. Pinti Paola, Scholkmann Felix, Hamilton Antonia, Burgess Paul, Tachtsidis Ilias, Current Status and Issues Regarding Pre-processing of fNIRS Neuroimaging Data: An Investigation of Diverse Signal Filtering Methods Within a General Linear Model Framework, Frontiers in Human Neuroscience, Volume 12, 2019, page 505, DOI=10.3389/fnhum.2018.00505    \n",
        "\t\n",
        "1. Aras, Roque & Abtahi, Mohammadreza & Mankodiya, Kunal. (2019). An end-to-end (deep) neural network applied to raw EEG, fNIRs and body motion data for data fusion and BCI classification task without any pre-/post-processing. \n",
        "\n",
        "1. Ma T, Chen W, Li X, Xia Y, Zhu X, He S. fNIRS Signal Classification Based on Deep Learning in Rock-Paper-Scissors Imagery Task. Applied Sciences. 2021; 11(11):4922. https://doi.org/10.3390/app11114922"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN06W4GOLOHy",
        "outputId": "d5d71d12-e4c7-48da-90b2-3871cf5db499"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install mne-nirs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mne-nirs\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6f8cc5facf5cd5011563671244ff0424cabde9d58d0e9b1f1c4262bf694a/mne-nirs-0.0.6.tar.gz\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from mne-nirs) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mne-nirs) (1.4.1)\n",
            "Collecting mne>=0.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f7/2bf5de3fad42b66d00ee27539bc3be0260b4e66fdecc12f740cdf2daf2e7/mne-0.23.0-py3-none-any.whl (6.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.0MB 12.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mne-nirs\n",
            "  Building wheel for mne-nirs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mne-nirs: filename=mne_nirs-0.0.6-cp37-none-any.whl size=54360 sha256=c162897a780ffb978260dac328c8ebc00d3ca5ad2b71922606d3e3285d73ff91\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/cf/a8/22a0ae5a59f13671a8313dc7c0ffd91eb9c671c372c9eba1ef\n",
            "Successfully built mne-nirs\n",
            "Installing collected packages: mne, mne-nirs\n",
            "Successfully installed mne-0.23.0 mne-nirs-0.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVhnhAU3NFh5"
      },
      "source": [
        "\n",
        "\n",
        "# Preprocessing functional near-infrared spectroscopy (fNIRS) data\n",
        "\n",
        "This tutorial covers how to convert functional near-infrared spectroscopy\n",
        "(fNIRS) data from raw measurements to relative oxyhaemoglobin (HbO) and\n",
        "deoxyhaemoglobin (HbR) concentration.\n",
        "\n",
        "Here we will work with the `fNIRS motor data <fnirs-motor-dataset>`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545,
          "referenced_widgets": [
            "99b165564b3c4bbf9571786a1c617de2",
            "9f39e6ec12bd46fc89fbef50c456fa73",
            "e2f97557cec942cda6265d28ad06c1be",
            "903a22f9a66a496bb28a9a4c09ae7a4e",
            "5c34233762fe47d88cf2bb82c2806b68",
            "007f2a4161ac43818715098745f0c756",
            "4fda4eb1c8a34c0db7c883f1c765e112",
            "baf25102ac654f6284f1f7bc73e9791e"
          ]
        },
        "id": "YIkAjzn1NFh6",
        "outputId": "15b10d6f-c821-4412-e03d-0296e030666e"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import compress\n",
        "\n",
        "import mne\n",
        "\n",
        "\n",
        "fnirs_data_folder = mne.datasets.fnirs_motor.data_path()\n",
        "fnirs_cw_amplitude_dir = os.path.join(fnirs_data_folder, 'Participant-1')\n",
        "raw_intensity = mne.io.read_raw_nirx(fnirs_cw_amplitude_dir, verbose=True)\n",
        "raw_intensity.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using default location ~/mne_data for fnirs_motor...\n",
            "Creating ~/mne_data\n",
            "Downloading archive MNE-fNIRS-motor-data.tgz to /root/mne_data\n",
            "Downloading https://files.osf.io/v1/resources/rxvq7/providers/osfstorage/5dbf84a9cfc96c000ec957eb?version=1&action=download&direct (17.1 MB)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99b165564b3c4bbf9571786a1c617de2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=17881709.0, style=ProgressStyle(descripâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Verifying hash c4935d19ddab35422a69f3326a01fef8.\n",
            "Decompressing the archive: /root/mne_data/MNE-fNIRS-motor-data.tgz\n",
            "(please be patient, this can take some time)\n",
            "Successfully extracted to: ['/root/mne_data/MNE-fNIRS-motor-data']\n",
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Loading /root/mne_data/MNE-fNIRS-motor-data/Participant-1\n",
            "Reading 0 ... 23238  =      0.000 ...  2974.464 secs...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "\n",
              "<table class=\"table table-hover\">\n",
              "    <tr>\n",
              "        <th>Measurement date</th>\n",
              "        <td>November 02, 2019  13:16:16 GMT</td>\n",
              "        \n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Experimenter</th>\n",
              "<td>Unknown</td>\n",
              "    </tr>\n",
              "        <th>Participant</th>\n",
              "        \n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Digitized points</th>\n",
              "        <td>31 points</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Good channels</th>\n",
              "        <td>0 magnetometer, 0 gradiometer,\n",
              "            and 0 EEG channels</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Bad channels</th>\n",
              "        <td></td>\n",
              "        \n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>EOG channels</th>\n",
              "        <td>Not available</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>ECG channels</th>\n",
              "        <td>Not available</td>\n",
              "    <tr>\n",
              "        <th>Sampling frequency</th>\n",
              "        <td>7.81 Hz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Highpass</th>\n",
              "        <td>0.00 Hz</td>\n",
              "    </tr>\n",
              "     <tr>\n",
              "        <th>Lowpass</th>\n",
              "        <td>3.91 Hz</td>\n",
              "    </tr>\n",
              "\n",
              "    <tr>\n",
              "        <th>Filenames</th>\n",
              "        <td>Participant-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Duration</th>\n",
              "        <td>00:49:34 (HH:MM:SS)</td>\n",
              "    </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<RawNIRX | Participant-1, 56 x 23239 (2974.5 s), ~10.0 MB, data loaded>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WoWKwz4NFh7"
      },
      "source": [
        "## View location of sensors over brain surface\n",
        "\n",
        "Here we validate that the location of sources-detector pairs and channels\n",
        "are in the expected locations. Source-detector pairs are shown as lines\n",
        "between the optodes, channels (the mid point of source-detector pairs) are\n",
        "optionally shown as orange dots. Source are optionally shown as red dots and\n",
        "detectors as black.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MukbWOM3Nqow",
        "outputId": "00706eb7-d096-424c-87f6-49b11e987bf3"
      },
      "source": [
        "!pip install mayavi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mayavi\n",
            "  Using cached https://files.pythonhosted.org/packages/e9/0c/4443d5690cebaa5f20ce9d092627932c1ef38c20c925b26d153615f56b6f/mayavi-4.7.3.tar.gz\n",
            "Requirement already satisfied: apptools in /usr/local/lib/python3.7/dist-packages (from mayavi) (5.1.0)\n",
            "Requirement already satisfied: envisage in /usr/local/lib/python3.7/dist-packages (from mayavi) (6.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mayavi) (1.19.5)\n",
            "Requirement already satisfied: pyface>=6.1.1 in /usr/local/lib/python3.7/dist-packages (from mayavi) (7.3.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from mayavi) (2.6.1)\n",
            "Requirement already satisfied: traits>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from mayavi) (6.2.0)\n",
            "Requirement already satisfied: traitsui>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from mayavi) (7.2.1)\n",
            "Requirement already satisfied: vtk in /usr/local/lib/python3.7/dist-packages (from mayavi) (9.0.3)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.7/dist-packages (from apptools->mayavi) (5.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from envisage->mayavi) (57.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from pyface>=6.1.1->mayavi) (4.6.0)\n",
            "Requirement already satisfied: importlib-resources>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pyface>=6.1.1->mayavi) (5.2.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from vtk->mayavi) (3.2.2)\n",
            "Requirement already satisfied: Twisted>=17.5.0 in /usr/local/lib/python3.7/dist-packages (from vtk->mayavi) (21.2.0)\n",
            "Requirement already satisfied: autobahn>=17.7.1 in /usr/local/lib/python3.7/dist-packages (from vtk->mayavi) (21.3.1)\n",
            "Requirement already satisfied: wslink>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from vtk->mayavi) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from configobj->apptools->mayavi) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pyface>=6.1.1->mayavi) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pyface>=6.1.1->mayavi) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk->mayavi) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk->mayavi) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk->mayavi) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk->mayavi) (0.10.0)\n",
            "Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk->mayavi) (5.4.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk->mayavi) (15.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk->mayavi) (21.2.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk->mayavi) (20.2.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk->mayavi) (21.0.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk->mayavi) (21.3.0)\n",
            "Requirement already satisfied: cryptography>=3.4.6 in /usr/local/lib/python3.7/dist-packages (from autobahn>=17.7.1->vtk->mayavi) (3.4.7)\n",
            "Requirement already satisfied: txaio>=21.2.1 in /usr/local/lib/python3.7/dist-packages (from autobahn>=17.7.1->vtk->mayavi) (21.2.1)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=17.5.0->vtk->mayavi) (2.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.4.6->autobahn>=17.7.1->vtk->mayavi) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.4.6->autobahn>=17.7.1->vtk->mayavi) (2.20)\n",
            "Building wheels for collected packages: mayavi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542,
          "referenced_widgets": [
            "cde927b920904b8ab5904b8b77c6d0bc",
            "ad81f9f2e8aa4b61a43932ccfe6afd00",
            "4348776d41c44de6b379392931474f11",
            "696d80e0592e4626a0df418fccb5b059",
            "05b235d067ac448a86a39cc8e86bb8e1",
            "0c3baec7e8714ee9891adf613536b345",
            "aeb1b0b59831423d89489a346a19ebc9",
            "a546faeafae94e5ab807f7c94b8c4bd7"
          ]
        },
        "id": "saSxJ4QLNFh7",
        "outputId": "2dca2878-5277-430b-99ed-7742eec867fd"
      },
      "source": [
        "subjects_dir = mne.datasets.sample.data_path() + '/subjects'\n",
        "\n",
        "fig = mne.viz.create_3d_figure(size=(800, 600), bgcolor='white')\n",
        "fig = mne.viz.plot_alignment(raw_intensity.info, show_axes=True,\n",
        "                             subject='fsaverage', coord_frame='mri',\n",
        "                             trans='fsaverage', surfaces=['brain'],\n",
        "                             fnirs=['channels', 'pairs',\n",
        "                                    'sources', 'detectors'],\n",
        "                             subjects_dir=subjects_dir, fig=fig)\n",
        "mne.viz.set_3d_view(figure=fig, azimuth=20, elevation=60, distance=0.4,\n",
        "                    focalpoint=(0., -0.01, 0.02))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using default location ~/mne_data for sample...\n",
            "Downloading archive MNE-sample-data-processed.tar.gz to /root/mne_data\n",
            "Downloading https://files.osf.io/v1/resources/rxvq7/providers/osfstorage/59c0e26f9ad5a1025c4ab159?version=5&action=download&direct (1.54 GB)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cde927b920904b8ab5904b8b77c6d0bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1652769680.0, style=ProgressStyle(descrâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Verifying hash 12b75d1cb7df9dfb4ad73ed82f61094f.\n",
            "Decompressing the archive: /root/mne_data/MNE-sample-data-processed.tar.gz\n",
            "(please be patient, this can take some time)\n",
            "Successfully extracted to: ['/root/mne_data/MNE-sample-data']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d648eeea22b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msubjects_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/subjects'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_3d_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbgcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m fig = mne.viz.plot_alignment(raw_intensity.info, show_axes=True,\n\u001b[1;32m      5\u001b[0m                              \u001b[0msubject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fsaverage'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mri'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mne/viz/backends/renderer.py\u001b[0m in \u001b[0;36mcreate_3d_figure\u001b[0;34m(size, bgcolor, smooth_shading, handle, scene)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mbgcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbgcolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msmooth_shading\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmooth_shading\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mne/viz/backends/renderer.py\u001b[0m in \u001b[0;36m_get_renderer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mset_3d_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_3d_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mne/viz/backends/renderer.py\u001b[0m in \u001b[0;36m_get_3d_backend\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 raise RuntimeError(f'Could not load any valid 3D backend: '\n\u001b[0m\u001b[1;32m    154\u001b[0m                                    f'{\", \".join(VALID_3D_BACKENDS)}')\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Could not load any valid 3D backend: pyvista, mayavi, notebook"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cER1OZbqNFh8"
      },
      "source": [
        "## Selecting channels appropriate for detecting neural responses\n",
        "\n",
        "First we remove channels that are too close together (short channels) to\n",
        "detect a neural response (less than 1 cm distance between optodes).\n",
        "These short channels can be seen in the figure above.\n",
        "To achieve this we pick all the channels that are not considered to be short.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0DdnVRSNFh8"
      },
      "source": [
        "picks = mne.pick_types(raw_intensity.info, meg=False, fnirs=True)\n",
        "dists = mne.preprocessing.nirs.source_detector_distances(\n",
        "    raw_intensity.info, picks=picks)\n",
        "raw_intensity.pick(picks[dists > 0.01])\n",
        "raw_intensity.plot(n_channels=len(raw_intensity.ch_names),\n",
        "                   duration=500, show_scrollbars=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga0dSNrMNFh9"
      },
      "source": [
        "## Converting from raw intensity to optical density\n",
        "\n",
        "The raw intensity values are then converted to optical density.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWISMz7ZNFh9"
      },
      "source": [
        "raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
        "raw_od.plot(n_channels=len(raw_od.ch_names),\n",
        "            duration=500, show_scrollbars=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb3U3M2CNFh-"
      },
      "source": [
        "## Evaluating the quality of the data\n",
        "\n",
        "At this stage we can quantify the quality of the coupling\n",
        "between the scalp and the optodes using the scalp coupling index. This\n",
        "method looks for the presence of a prominent synchronous signal in the\n",
        "frequency range of cardiac signals across both photodetected signals.\n",
        "\n",
        "In this example the data is clean and the coupling is good for all\n",
        "channels, so we will not mark any channels as bad based on the scalp\n",
        "coupling index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IPH5oOlNFh-"
      },
      "source": [
        "sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(sci)\n",
        "ax.set(xlabel='Scalp Coupling Index', ylabel='Count', xlim=[0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8MpJJfjNFh_"
      },
      "source": [
        "In this example we will mark all channels with a SCI less than 0.5 as bad\n",
        "(this dataset is quite clean, so no channels are marked as bad).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3erpjoWNFh_"
      },
      "source": [
        "raw_od.info['bads'] = list(compress(raw_od.ch_names, sci < 0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mprTCmZMNFh_"
      },
      "source": [
        "At this stage it is appropriate to inspect your data\n",
        "(for instructions on how to use the interactive data visualisation tool\n",
        "see `tut-visualize-raw`)\n",
        "to ensure that channels with poor scalp coupling have been removed.\n",
        "If your data contains lots of artifacts you may decide to apply\n",
        "artifact reduction techniques as described in `ex-fnirs-artifacts`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yd7ht2_NFiA"
      },
      "source": [
        "## Converting from optical density to haemoglobin\n",
        "\n",
        "Next we convert the optical density data to haemoglobin concentration using\n",
        "the modified Beer-Lambert law.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVwuTDwqNFiA"
      },
      "source": [
        "raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od)\n",
        "raw_haemo.plot(n_channels=len(raw_haemo.ch_names),\n",
        "               duration=500, show_scrollbars=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUBvhusLNFiA"
      },
      "source": [
        "## Removing heart rate from signal\n",
        "\n",
        "The haemodynamic response has frequency content predominantly below 0.5 Hz.\n",
        "An increase in activity around 1 Hz can be seen in the data that is due to\n",
        "the person's heart beat and is unwanted. So we use a low pass filter to\n",
        "remove this. A high pass filter is also included to remove slow drifts\n",
        "in the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l8kR7LsNFiA"
      },
      "source": [
        "fig = raw_haemo.plot_psd(average=True)\n",
        "fig.suptitle('Before filtering', weight='bold', size='x-large')\n",
        "fig.subplots_adjust(top=0.88)\n",
        "raw_haemo = raw_haemo.filter(0.05, 0.7, h_trans_bandwidth=0.2,\n",
        "                             l_trans_bandwidth=0.02)\n",
        "fig = raw_haemo.plot_psd(average=True)\n",
        "fig.suptitle('After filtering', weight='bold', size='x-large')\n",
        "fig.subplots_adjust(top=0.88)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BvmuBGpNFiB"
      },
      "source": [
        "## Extract epochs\n",
        "\n",
        "Now that the signal has been converted to relative haemoglobin concentration,\n",
        "and the unwanted heart rate component has been removed, we can extract epochs\n",
        "related to each of the experimental conditions.\n",
        "\n",
        "First we extract the events of interest and visualise them to ensure they are\n",
        "correct.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY_8jTkTNFiB"
      },
      "source": [
        "events, _ = mne.events_from_annotations(raw_haemo, event_id={'1.0': 1,\n",
        "                                                             '2.0': 2,\n",
        "                                                             '3.0': 3})\n",
        "event_dict = {'Control': 1, 'Tapping/Left': 2, 'Tapping/Right': 3}\n",
        "fig = mne.viz.plot_events(events, event_id=event_dict,\n",
        "                          sfreq=raw_haemo.info['sfreq'])\n",
        "fig.subplots_adjust(right=0.7)  # make room for the legend"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVylIXwGNFiB"
      },
      "source": [
        "Next we define the range of our epochs, the rejection criteria,\n",
        "baseline correction, and extract the epochs. We visualise the log of which\n",
        "epochs were dropped.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkePpU8bNFiB"
      },
      "source": [
        "reject_criteria = dict(hbo=80e-6)\n",
        "tmin, tmax = -5, 15\n",
        "\n",
        "epochs = mne.Epochs(raw_haemo, events, event_id=event_dict,\n",
        "                    tmin=tmin, tmax=tmax,\n",
        "                    reject=reject_criteria, reject_by_annotation=True,\n",
        "                    proj=True, baseline=(None, 0), preload=True,\n",
        "                    detrend=None, verbose=True)\n",
        "epochs.plot_drop_log()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Culc1ccGNFiC"
      },
      "source": [
        "## View consistency of responses across trials\n",
        "\n",
        "Now we can view the haemodynamic response for our tapping condition.\n",
        "We visualise the response for both the oxy- and deoxyhaemoglobin, and\n",
        "observe the expected peak in HbO at around 6 seconds consistently across\n",
        "trials, and the consistent dip in HbR that is slightly delayed relative to\n",
        "the HbO peak.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV2Na6EbNFiC"
      },
      "source": [
        "epochs['Tapping'].plot_image(combine='mean', vmin=-30, vmax=30,\n",
        "                             ts_args=dict(ylim=dict(hbo=[-15, 15],\n",
        "                                                    hbr=[-15, 15])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCO6ieSnNFiC"
      },
      "source": [
        "We can also view the epoched data for the control condition and observe\n",
        "that it does not show the expected morphology.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kkcYGnqNFiC"
      },
      "source": [
        "epochs['Control'].plot_image(combine='mean', vmin=-30, vmax=30,\n",
        "                             ts_args=dict(ylim=dict(hbo=[-15, 15],\n",
        "                                                    hbr=[-15, 15])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s1D1RywNFiC"
      },
      "source": [
        "## View consistency of responses across channels\n",
        "\n",
        "Similarly we can view how consistent the response is across the optode\n",
        "pairs that we selected. All the channels in this data are located over the\n",
        "motor cortex, and all channels show a similar pattern in the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVWim3GhNFiD"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 6))\n",
        "clims = dict(hbo=[-20, 20], hbr=[-20, 20])\n",
        "epochs['Control'].average().plot_image(axes=axes[:, 0], clim=clims)\n",
        "epochs['Tapping'].average().plot_image(axes=axes[:, 1], clim=clims)\n",
        "for column, condition in enumerate(['Control', 'Tapping']):\n",
        "    for ax in axes[:, column]:\n",
        "        ax.set_title('{}: {}'.format(condition, ax.get_title()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl6uP4yuNFiD"
      },
      "source": [
        "## Plot standard fNIRS response image\n",
        "\n",
        "Next we generate the most common visualisation of fNIRS data: plotting\n",
        "both the HbO and HbR on the same figure to illustrate the relation between\n",
        "the two signals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYZOQeGPNFiD"
      },
      "source": [
        "evoked_dict = {'Tapping/HbO': epochs['Tapping'].average(picks='hbo'),\n",
        "               'Tapping/HbR': epochs['Tapping'].average(picks='hbr'),\n",
        "               'Control/HbO': epochs['Control'].average(picks='hbo'),\n",
        "               'Control/HbR': epochs['Control'].average(picks='hbr')}\n",
        "\n",
        "# Rename channels until the encoding of frequency in ch_name is fixed\n",
        "for condition in evoked_dict:\n",
        "    evoked_dict[condition].rename_channels(lambda x: x[:-4])\n",
        "\n",
        "color_dict = dict(HbO='#AA3377', HbR='b')\n",
        "styles_dict = dict(Control=dict(linestyle='dashed'))\n",
        "\n",
        "mne.viz.plot_compare_evokeds(evoked_dict, combine=\"mean\", ci=0.95,\n",
        "                             colors=color_dict, styles=styles_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHcHUT2LNFiD"
      },
      "source": [
        "## View topographic representation of activity\n",
        "\n",
        "Next we view how the topographic activity changes throughout the response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5LpmbfYNFiD"
      },
      "source": [
        "times = np.arange(-3.5, 13.2, 3.0)\n",
        "topomap_args = dict(extrapolate='local')\n",
        "epochs['Tapping'].average(picks='hbo').plot_joint(\n",
        "    times=times, topomap_args=topomap_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmxc1gn5NFiD"
      },
      "source": [
        "## Compare tapping of left and right hands\n",
        "\n",
        "Finally we generate topo maps for the left and right conditions to view\n",
        "the location of activity. First we visualise the HbO activity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttQfJy5dNFiE"
      },
      "source": [
        "times = np.arange(4.0, 11.0, 1.0)\n",
        "epochs['Tapping/Left'].average(picks='hbo').plot_topomap(\n",
        "    times=times, **topomap_args)\n",
        "epochs['Tapping/Right'].average(picks='hbo').plot_topomap(\n",
        "    times=times, **topomap_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrF998hNNFiE"
      },
      "source": [
        "And we also view the HbR activity for the two conditions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTaYfraaNFiE"
      },
      "source": [
        "epochs['Tapping/Left'].average(picks='hbr').plot_topomap(\n",
        "    times=times, **topomap_args)\n",
        "epochs['Tapping/Right'].average(picks='hbr').plot_topomap(\n",
        "    times=times, **topomap_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ5w5srcNFiE"
      },
      "source": [
        "And we can plot the comparison at a single time point for two conditions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSlnIJ40NFiE"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(9, 5),\n",
        "                         gridspec_kw=dict(width_ratios=[1, 1, 1, 0.1]))\n",
        "vmin, vmax, ts = -8, 8, 9.0\n",
        "\n",
        "evoked_left = epochs['Tapping/Left'].average()\n",
        "evoked_right = epochs['Tapping/Right'].average()\n",
        "\n",
        "evoked_left.plot_topomap(ch_type='hbo', times=ts, axes=axes[0, 0],\n",
        "                         vmin=vmin, vmax=vmax, colorbar=False,\n",
        "                         **topomap_args)\n",
        "evoked_left.plot_topomap(ch_type='hbr', times=ts, axes=axes[1, 0],\n",
        "                         vmin=vmin, vmax=vmax, colorbar=False,\n",
        "                         **topomap_args)\n",
        "evoked_right.plot_topomap(ch_type='hbo', times=ts, axes=axes[0, 1],\n",
        "                          vmin=vmin, vmax=vmax, colorbar=False,\n",
        "                          **topomap_args)\n",
        "evoked_right.plot_topomap(ch_type='hbr', times=ts, axes=axes[1, 1],\n",
        "                          vmin=vmin, vmax=vmax, colorbar=False,\n",
        "                          **topomap_args)\n",
        "\n",
        "evoked_diff = mne.combine_evoked([evoked_left, evoked_right], weights=[1, -1])\n",
        "\n",
        "evoked_diff.plot_topomap(ch_type='hbo', times=ts, axes=axes[0, 2:],\n",
        "                         vmin=vmin, vmax=vmax, colorbar=True,\n",
        "                         **topomap_args)\n",
        "evoked_diff.plot_topomap(ch_type='hbr', times=ts, axes=axes[1, 2:],\n",
        "                         vmin=vmin, vmax=vmax, colorbar=True,\n",
        "                         **topomap_args)\n",
        "\n",
        "for column, condition in enumerate(\n",
        "        ['Tapping Left', 'Tapping Right', 'Left-Right']):\n",
        "    for row, chroma in enumerate(['HbO', 'HbR']):\n",
        "        axes[row, column].set_title('{}: {}'.format(chroma, condition))\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLHaI8IpNFiF"
      },
      "source": [
        "Lastly, we can also look at the individual waveforms to see what is\n",
        "driving the topographic plot above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRX194gnNFiF"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
        "mne.viz.plot_evoked_topo(epochs['Left'].average(picks='hbo'), color='b',\n",
        "                         axes=axes, legend=False)\n",
        "mne.viz.plot_evoked_topo(epochs['Right'].average(picks='hbo'), color='r',\n",
        "                         axes=axes, legend=False)\n",
        "\n",
        "# Tidy the legend.\n",
        "leg_lines = [line for line in axes.lines if line.get_c() == 'b'][:1]\n",
        "leg_lines.append([line for line in axes.lines if line.get_c() == 'r'][0])\n",
        "fig.legend(leg_lines, ['Left', 'Right'], loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}