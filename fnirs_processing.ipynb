{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fnirs_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99b165564b3c4bbf9571786a1c617de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f39e6ec12bd46fc89fbef50c456fa73",
              "IPY_MODEL_e2f97557cec942cda6265d28ad06c1be"
            ],
            "layout": "IPY_MODEL_903a22f9a66a496bb28a9a4c09ae7a4e"
          }
        },
        "9f39e6ec12bd46fc89fbef50c456fa73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c34233762fe47d88cf2bb82c2806b68",
            "max": 17881709,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_007f2a4161ac43818715098745f0c756",
            "value": 17881709
          }
        },
        "e2f97557cec942cda6265d28ad06c1be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fda4eb1c8a34c0db7c883f1c765e112",
            "placeholder": "​",
            "style": "IPY_MODEL_baf25102ac654f6284f1f7bc73e9791e",
            "value": " Downloading : 17.1M/17.1M [00:01&lt;00:00,    10.8MB/s]"
          }
        },
        "903a22f9a66a496bb28a9a4c09ae7a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c34233762fe47d88cf2bb82c2806b68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "007f2a4161ac43818715098745f0c756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4fda4eb1c8a34c0db7c883f1c765e112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baf25102ac654f6284f1f7bc73e9791e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cde927b920904b8ab5904b8b77c6d0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad81f9f2e8aa4b61a43932ccfe6afd00",
              "IPY_MODEL_4348776d41c44de6b379392931474f11"
            ],
            "layout": "IPY_MODEL_696d80e0592e4626a0df418fccb5b059"
          }
        },
        "ad81f9f2e8aa4b61a43932ccfe6afd00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05b235d067ac448a86a39cc8e86bb8e1",
            "max": 1652769680,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c3baec7e8714ee9891adf613536b345",
            "value": 1652769680
          }
        },
        "4348776d41c44de6b379392931474f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aeb1b0b59831423d89489a346a19ebc9",
            "placeholder": "​",
            "style": "IPY_MODEL_a546faeafae94e5ab807f7c94b8c4bd7",
            "value": " Downloading : 1.54G/1.54G [00:50&lt;00:00,    32.7MB/s]"
          }
        },
        "696d80e0592e4626a0df418fccb5b059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05b235d067ac448a86a39cc8e86bb8e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c3baec7e8714ee9891adf613536b345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "aeb1b0b59831423d89489a346a19ebc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a546faeafae94e5ab807f7c94b8c4bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rogerio-mack/fNIRS/blob/main/fnirs_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XXMbfcRa8pZ"
      },
      "source": [
        "# **fNIRS**, uma Introdução: Princípios, Dispositivos, Tratamento e Classificação dos Sinais \n",
        "\n",
        "Autor: *rogerio.oliveira@mackenzie.br*\n",
        "\n",
        "Filiação: Universidade Presbiteriana Mackenzie\n",
        "\n",
        "Data: 2021-09-20\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRQGfjpOcNlP"
      },
      "source": [
        "## Abstract\n",
        "\n",
        "Esta revisão é uma introdução à espectroscopia funcional de infravermelho próximo (fNIRS) para seu uso prático, em especial em neurociências. Ela aborda desde os princípios gerais da espectroscopia de infravermelho próximo até a construção de modelos simples de mapeamento funcional. Fornece assim, uma visão geral do uso de fNIRS para os interessados no processamento de imagens e sinais cerebrais aplicados à neurociência como também um primeiro guia prático para experimentação e uso de softwares livres de análise. Ao final indicamos algumas das potenciais vantagens deste roteiro para o uso de técnicas de análise dos sinais fNIRS ainda pouco explorados.\n",
        "\n",
        "**Palavras-Chave**: *fNIRS, espectroscopia funcional de infravermelho próximo, neurociências, neuroimagem, deep learning*  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3ATZFWQgCdk"
      },
      "source": [
        "## Introdução\n",
        "\n",
        "Este estudo fornece aos interessados em neurociências, e em particular aos interessados no uso prático de neuroimagens, uma visão geral da espectroscopia funcional de infravermelho próximo (fNIRS) para o desenvolvimento de experimentos e análise de imagens cerebrais.\n",
        "\n",
        "Desde seu surgimento na década de 80 espectroscopia funcional de infravermelho próximo (fNIRS) vem se mostrando como uma alternativa viável, não invasiva, versátil e de baixo custo (*) quando comparados a outras técnicas como a tomografia por emissão positrons (PET e SPECT), magnetoencefalografia (MEG) e ressonância magnética funcional (fMRI) [cite].  Assim como as técnicas de PET, SPECT e fMRI, o fNRIS é um método indireto que mede a atividade cerebral a partir das alterações da homodinâmica cerebral no que se diferencia de outros métodos que medem diretamente a atividade elétrica do cérebro como o MEG e EEG (eletroencelografia) [cite]. A fNIRS vem desempenhando um papel importante no mapeamento das funções do cerébro [cite], e seu baixo custo e mobilidade abrem espaço para muitas novas aplicações clínicas [cite] e de interface homem-máquina [cite].\n",
        "\n",
        "O uso prático da fNIRS requer, entretanto, uma espectro amplo de conhecimentos que vão desde o sistema de fNIRS empregado e o desenho correto do experimento para a captura dos dados, até o emprego de várias técnicas para o tratamento e análise do sinais [cite]. Em conjunto com técnicas estatísticas mais tradicionais, como modelo lineares generalizados, modelos de aprendizado de máquina vem sendo empregados para o tratamento e classificação dos sinais e,mais recentemente, modelos de redes neurais profundas parecem permitir o emprego de direto de dados brutos de fNIRS [cite]. \n",
        "\n",
        "Dadas as diferentes aplicações que se pode dar a fNIRS, em especial as que envolvem mobilidade e aplicações interface homem-máquina, e variedade de técnicas e usos potenciais dos sinais de fNIRS para essas aplicações, é importante que as ferramentas para uso dessas técnicas sejam o mais possível abertas e acessíveis, permitindo maior flexibilidade de pesquisa e construção de aplicações e novos métodos. Nesse sentido, o uso de soluções de código aberto (como também de hardware) parece ser a alternativa mais adequada. Nessa perpectiva este estudo contribui apresentando um roteiro prático, da aquisição até o processamento dos sinais fNIRS, empregando soluções de código aberto para pesquisadores e interessados de quaisquer áreas. \n",
        "\n",
        "O estudo encontra-se organizado como segue. A primeira seção descreve brevemente as bases biológicas e físicas da espectroscopia funcional de infravermelho próximo (fNIRS). Em seguida são descritos os diferentes sistemas de fNIRS, seus tipos de iluminação e coleta de dados. A seção três apresenta algumas aplicações dessa téncnica e discute vantagens e limitações do seu uso. As seções seguintes apresentam um roteiro prático de análise dos dados de fNIRS, desde a aquisição e o tratamento dos sinais, até o emprego de técnicas de aprendizado de máquina para a classificação dos sinais com o emprego softwares livres. Ao final indicamos vantagens do roteiro proposto indicando possíveis ténicas de análise ainda pouco exploradas.\n",
        "\n",
        "De modo a proporcionar fácil acesso ao uso desse tutorial todo o código e dados encontram-se disponíveis publicamente no `https://github.com/Rogerio-mack` e, embora o estudo esteja focado em aplicações de neurociência boa parte dos temas aqui tratados podem ser igualmente empregados em outras aplicações da espectroscopia de infravermelho próximo, como outras aplicações em saúde (oxigenação de músculos e outros órgãos) e engenharia de alimentos. \n",
        "\n",
        "<small> Baixo custo, [2] aponta a equipamentos que vão de USD 10.000 a USD 100.000. Uma pesquisa na internet indica haverem equipamentos comerciais móveis, pré-frontais, de até 8 emissores a partir USD 5.000, e um sistema análogo parece poder ser construído de modo aberto com cerca de USD 2.000 (~ USD 250 por canal [HW1]).\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M80sf4XGLEyT"
      },
      "source": [
        "## Princípios de fNIRS\n",
        "\n",
        "Além dos neurônios e células gliais, o suprimento de sangue no cérebro desempenha papel essencial no funcionamento do cérebro. Os neurônios não têm nenhuma provisão substancial de oxigênio ou glicose, e aumentos na atividade neural requerem um aumento no suprimento de sangue. Essa relação entre a atividade neural e a hemodinâmica cerebral é denominada de acoplamento neurovascular. Embora não seja totalmente compreendido, o acoplamento neurovascular fornece um sinal indireto da atividade cerebral e é o princípio de sistemas de neuroimagem como o fMRI e fNIRS. Essa dependência comum pelo princípio do acoplamento neurovascular permite que as duas técnicas compartilhem várias aplicações e métodos de análise.\n",
        "\n",
        "Enquanto os sinais fMRI (BOLD ou Blood-oxygen-level-dependent imaging) medem mudanças na suscetibilidade magnética do sangue fornecendo uma medida da atividade cerebral, o fNIRS explora as propriedades ópticas dos tecidos, principalmente do sangue, medindo a absorção da luz na faixa NIR para inferir sobre a atividade cerebral. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeX90ANn3TGZ"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/Rogerio-mack/fNIRS/main/images/Fig_Blood_Oxi_Coupling.png\" width=800, align=\"center\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuT5DA77q_Fp"
      },
      "source": [
        "<small> <b> Fig. 1. Esquemático de um vaso capilar do tecido cerebral durante o repouso (esquerda) e durante a atividade neural (direita). Os círculos vermelhos e azuis representam respectivamente os glóbulos vermelhos totalmente oxigenados (oxi-hemoglobina, HbO2) e totalmente desoxigenados (Hb, desoxi-hemoglobina) (fonte: Glover [4])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZJlNcv_f7E6"
      },
      "source": [
        "Em princípio os sinais para neuroimagem de fNIRS seguem o mesmo princípio de um oximetro comum. O sinal de luz atravessa os tecidos e sinal resultante é medido por um foto diodo. A diferença do sinal emitido e resultante depende de características físicas do meio em transmitir, absorver, refletir e dissipar os sinais e pode fornecer informações sobre as propriedades do tecido. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5r0bUEq5lHx"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_LED_and_Photodiodo.png?raw=true\" width=800, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKM2MxzAffh6"
      },
      "source": [
        "<small> <b> Fig. 2. Em princípio os sinais para neuroimagem de fNIRS seguem o mesmo princípio de um oximetro comum (imagens adaptadas de: https://protosupplies.com/product/heartbeat-sensor-module/ e https://www.roswellpark.org/cancertalk/202004/should-you-buy-pulse-oximeter-covid-19-question). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWYF0FU5XJHN"
      },
      "source": [
        "### Propriedades ópticas\n",
        "\n",
        "O fNIRS emprega luz infravermelha próxima de comprimentos de onda maiores do que o visível (750 nm - 1200 nm). A propagação da luz depende do comprimento de onda e de muitas propriedades ópticas e físicas do meio (constituição química e consistência) e do sinal emitido (ângulo de incidência) que determinam a transmissão, absorção, reflexão e dissipação da onda. \n",
        "\n",
        "Os grupos que absorvem luz em comprimentos de onda específicos são a hemoglobina oxigenada HbO$_2$, a hemoglobina desoxigenada Hb e sua soma, hemoglobina total HbT, além do Citocromo c oxidase (Caa$_3$), e suas concentrações mudam com o tempo e com a concentração de oxigênio. A faixa de luz  de 750 nm - 1200 nm fornece uma *janela óptica* interessante para os sinais de fNIRS pois apresenta uma absorção predominante do grupo da hemoglobina (oxigenada, desoxigenada e total). \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2iPAfOc56cD"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_Light_Waves_Absortion.png?raw=true\" width=500, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfD5c9HEsw44"
      },
      "source": [
        "<small> <b> Fig. 3. Espectro de absorção de Hb, HbO2, H2O e outros cromóforos na faixa NIR. O ponto isosbéstico do espectro de absorção de HB / HbO 2 é circulado em branco dentro da janela óptica do NIR. (fonte: [1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO7byOB4etPs"
      },
      "source": [
        "A escolha dos comprimentos de onda ótimos, dois ou mais, a serem empregados é uma das etapas de desenho do experimento [R2]. Um uso comum é o emprego de dois comprimentos, um entre 650-730nm e outro ~830nm. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQouec6PvBUY"
      },
      "source": [
        "## Determinando HbO2 e Hb\n",
        "\n",
        "As concentrações de oxi-hemoglobina, desoxi-hemoglobina e total não podem ser medidas diretamente, mas podem ser calculadas a partir dos dados brutos das quantidades de intensidade de luz  medidas no experimento. A absorção de luz é medida pelo coeficiente de extinção molar $A$ em função do comprimento de onda e existem várias formas para se converter esses dados brutos nas concentrações de oxi-hemoglobina, desoxi-hemoglobina e total. A forma mais comum em fNIRS é o emprego da Lei Modificada de Beer-Lambert (MBLL, Modified Beer-Lambert Law) será o procedimento empregado aqui mais adiante. A MBLL estabelece que a mudança na atenuação da luz é proporcional às mudanças nas concentrações dos cromóforos do tecido, principalmente oxi e desoxi-hemoglobina e permite calcular suas concentrações a partir das mudanças de atenuação medidas em dois ou mais comprimentos de onda assumindo um meio homogêneo em que a absorção do  tecido muda de maneira homogênea e a difusão é constante [R5]. Essas premissas não são totalmente verdadeiras para o tecido cerebral, mas essa abordagem é bastante empregada não parece afetar significativamente os resultados qualitativos [R2]. \n",
        "\n",
        "### Lei Modificada de Beer-Lambert \n",
        "\n",
        "A lei de Beer-Lambert relaciona a atenuação da luz por absorção à concentração de cromóforo [R5]:\n",
        "\n",
        "\\begin{align*} A = ln( \\frac{I_i}{I_d} ) = L \\mu_a +G, \\end{align*}\n",
        "\n",
        "onde $A$ é a atenuação do sinal de luz, $I_i$ e $I_d$ são respectivamente intensidade de luz incidente e a detectada, $\\mu_a$ é o coeficiente de absorção do cromóforo e $L$ a distância média entre a fonte de luz e o detector. $G$ é um fator dependente da geometria dos optodos e envolve a perda de intensidade causada pelo espalhamento. Se considerarmos que a absorção muda homogeneamente e $G$ constante, podemos eliminar $G$ e obter as diferenças de atenuação como : \n",
        "\n",
        "\\begin{align*} \\Delta A = ln( \\frac{I_d^1}{I_d^2} ) = L \\Delta \\mu_a, \\end{align*}\n",
        "\n",
        "Assim, a diferença de atenuação é calculada a partir da intensidade de dois\n",
        "diferentes estados do tecido, e é proporcional à diferença de absorção que ainda pode ser reescrita com base nas concentração dos cromóforos de oxi e\n",
        "desoxi-hemoglobina:\n",
        "\n",
        "\\begin{align*} \\Delta \\mu_a = a_{HbO_2}. \\Delta C_{HbO_2} + a_{Hb}. \\Delta C_{Hb} \\end{align*}\n",
        "\n",
        "E cálculo das variações de concentração de oxi-hemoglobina, desoxi-hemoglobina para dois comprimentos de onda, $\\lambda_1$ e $\\lambda_2$, pela MBLL é dado então por:\n",
        "\n",
        "\\begin{align*} \\Delta C_{HbO_2} &= \\frac{a_{Hb}^{\\lambda _1}. \\Delta A_{\\lambda _2} / L_{\\lambda _2} - a_{Hb}^{\\lambda _2}. \\Delta A_{\\lambda _1} / L_{\\lambda _1}}{(a_{HbO_2}^{\\lambda _1}. a_{Hb}^{\\lambda _2} - a_{HbO_2}^{\\lambda _2}. a_{Hb}^{\\lambda _1}) }\n",
        "\\\\ \\Delta C_{Hb} &= \\frac{a_{HbO_2}^{\\lambda _1}. \\Delta A_{\\lambda _2} / L_{\\lambda _2} - a_{HbO_2}^{\\lambda _2}. \\Delta A_{\\lambda _1} / L_{\\lambda_1}} {(a_{HbO_2}^{\\lambda _1}. {a_{Hb}^{\\lambda _2} - a_{HbO_2}^{\\lambda _2}. a_{Hb}^{\\lambda _1}})} \\end{align*}\n",
        "\n",
        "Para mais de dois comprimentos de onda o cálculo requer o uso do método de mínimos quadrados ou técnicas semelhantes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmXdxUmL6Ucm"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_General_fNIRS.png?raw=true\" width=1100, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A2pf9SN0yIm"
      },
      "source": [
        "<small> <b> Fig. 4. Estágios de aquisição de dados fNIRS. (a) A luz é projetada ao longo de um caminho em forma de banana do optodo de luz através do couro cabeludo, crânio, líquido cefalorraquidiano e no córtex. A luz é absorvida,\n",
        "espalhados e refletidos da cabeça para o detector. (b) Mudanças na intensidade da luz estão relacionadas às mudanças de concentração em\n",
        "hemoglobina pela lei modificada de Beer-Lambert. Isso produz a função de resposta hemodinâmica (HRF). (c) Oxi-Hb\n",
        "é representado pela linha vermelha, enquanto a desoxi-Hb é representada pela linha azul. O eixo x é o tempo em segundos, e o\n",
        "eixo y é a mudança relativa da concentração dos cromóforos (adaptado de [T1])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdiRoE9rLika"
      },
      "source": [
        "## Diferentes Sistemas de fNIRS\n",
        "\n",
        "De modo simples e de um ponto de vista técnico um sistema fNIRS pode ser entendido como uma fonte de luz NIR, acoplada e emitida no couro cabeludo, e um detector que mede a refletância difusa do sinal que reemerge do tecido a\n",
        "a poucos centímetros da fonte. De qualquer modo os sistemas de fNIRS podem diferir em muitos aspectos como, a forma como são medidos os sinais de luz, tipo e quantidade de fontes empregados, comprimentos de onda e os tipos de detectores, e essas características tem forte impacto nas limitações e no alcance do experimento que se quer realizar.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mpo-SUdCfpt"
      },
      "source": [
        "##  Medidas do Sinal de Luz\n",
        "\n",
        "Existem três tipos principais de sistemas NIRS: \n",
        "\n",
        "1. Sistemas de Onda Contínua\n",
        "1. Espectrômetros de Domínio do Tempo \n",
        "1. Espectrômetros de Domínio da Freqüência\n",
        "\n",
        "Os sistemas de Onda Contínua são os mais antigos e mais comumente empregado comercialmente. Eles emitem luz em uma intensidade constante e então medem  apenas as mudanças na intensidade da luz refletida difusamente através do tecido com um fotodetector ou um fotodiodo, e empregam  dois ou mais comprimentos de onda emitidos por múltiplas fontes. Esses sistemas, entretanto, não conseguem separar a atenuação da absorção e do espalhamento e, portanto, não podemos obter as concentrações de oxi e desoxi-hemoglobina de forma absoluta e empregam a diferença relativa das concentrações.  Eles também pode apresentar baixa profundidade de penetração da luz devido à curta distância fonte-detector. Apesar disso, esses sistemas tem vantagens em simplicidade, tamanho, peso e custo quando comparados aos demais sistemas de domínio do tempo e da frequência, que medem não só a intensidade da luz, mas também o tempo que a luz leva para viajar através do tecido. Para essas medidas os sistemas são bastante mais complexos e podem envolver fontes de laser e medidores sensíveis de fotón unico. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il5Q6tJN6w9U"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_Signal_Types.png?raw=true\" width=800, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oheD72ICvDO5"
      },
      "source": [
        "<small> <b> Fig 5. Diferentes técnicas de medida do sinal (adaptado de [R2])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ9EgJ9L7mQR"
      },
      "source": [
        "## Emissores e Detectores\n",
        "\n",
        "Na maioria dos sistemas a fonte de luz é um diodo emissor de luz (LED) [R1, R2]. O LED é um diodo semicondutor que quando é energizado emite luz visível. A luz não é monocromática (como em um laser), mas consiste de uma banda espectral relativamente estreita e é produzida pelas interações energéticas do elétron. Outros sistemas empregam ainda diodos laser, mas os LEDs são frequentemente preferidos por razões de segurança [R1] (por exemplo, devido a problemas de aquecimento por uma exposição prolongada). Em quaisquer casos as faixas de onda  ficam entre 685-780 nm e 805-850 nm. \n",
        "\n",
        "Os detectores mais empregados são fotodiodos. Um fotodiodo é um dispositivo de junção pn semicondutor que converte luz em corrente elétrica. Há ainda sistemas que empregam fotodiodos de avalanche  e tubos fotomultiplicadores [R1, R2]. Em quaisquer casos uma preocupação no projeto e operação dos detectores é a necessidade de bloquear a luz ambiente uma vez que eles apresentam baixa seletividade de sinal [R1]. \n",
        "\n",
        "Tanto para os emissores como para os detectores uma preocupação importante no projeto e operação dos optodos é na interferência do cabelo para emissão e detecção dos sinais de luz sendo importante técnicas para melhor contato e ajuste dos optodos, como a capacidade de lidar com diferentes quantidades de cabelo, diferentes texturas de cabelo e diferentes cores para experimentos e sistemas inclusivos (https://research.fb.com/programs/research-awards/proposals/request-for-proposals-on-engineering-approaches-to-responsible-neural-interface-design/). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n63NobiiJ8oi"
      },
      "source": [
        "## Coleta dos dados \n",
        "\n",
        "A maior parte dos dispositivos fornece intensidades de luz brutas. Mas existem ainda poucos sistemas que fornecem densidades ópticas ou dados diretos das concentrações de oxi e desoxi-homoglobina o que não é adequado para o emprego de algoritmos customizados [R2]. \n",
        "\n",
        "Os dados em geral são amplificados e armazenados no mesmo dispositivo ou ainda transmitidos para um outro dispositivo para o tratamento dos dados (redução de ruído, transformações, análise). As soluções abertas em geral empregam um computador ou um celular com uso de software aberto ou scripts customizados para o tratamento dos dados, e aqui nossa opção é pelo uso de scripts em Ptyhon.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnDs2WUHNbiA"
      },
      "source": [
        "## Mobilidade\n",
        "\n",
        "Sistemas fNIRS podem ter a luz NIR direcionada diretamente para o couro cabeludo a partir da fonte NIR ou transmitida por fibras ópticas a partir de um dispositivo central. Do mesmo modo a luz NIR refletida é recebida da cabeça diretamente pelo detector NIR ou guiada por meio de fibras ópticas para um detector NIR central. Os sistemas do primeiro modo (direto) são sistemas vestíveis que permitem mobilidade do sujeito. São sistemas de menor capacidade mas que viabilizam uma série de experimentos que requerem a mobilidade do sujeito. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdzP3iGBvryb"
      },
      "source": [
        "## Arranjo dos Emissores e Detectores\n",
        "\n",
        "Um sensor fNIR possui dois componentes: um emissor e um detector. Cada par  forma um canal, um caminho entre uma fonte de luz e um detector. Por exemplo podemos ter um emissor e dois detectores para criar dois canais. Em geral o número de detectores é maior que o número de emissores e, na prática, o número de canais é bastante menor que o número de pares emissor-detector, uma vez que os canais utilizáveis não podem ter uma distância muito grande entre a fonte e o detector de luz o que também irá depender do arranjo dos optodos no crânio.\n",
        "\n",
        "De qualquer maneira a disposição dos emissores e detectores em geral seguem um de 3 tipos de esquemas:\n",
        "\n",
        "1. Transiluminação\n",
        "1. Refletância\n",
        "1. Refletância diferencial\n",
        "\n",
        "O esquema de transiluminação, pela distância do emissor-detector, só pode ser empregado para recém-nascidos, embora a técnica possa ser empregada para outros fins em outros orgãos do corpo humano. O modo de refletância é o mais utilizado nos dispositivos de fNIRS. A região sensível entre emissor-detector assume uma forma de *banana* fazendo com que a profundidade de penetração do NIRS dependa da distância do emissor-detector, sendo estimada em cerca de 1/3 dessa distância [R1]. Tipicamente empregam-se distâncias de 3-4 cm permitindo uma profundidade de 1-2 cm a depender da posição do optodo na cabeça e do sujeito [R1, R3] e muitos sistemas podem ter a distância fixa (não ajustável) no suporte que envolve os optodos. Na refletância diferencial mais um detector ou fonte são empregados para medir a diferença entre os caminhos de luz.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AznNo5NO7PwT"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_Emissor_Detector_Schemas.png?raw=true\" width=600, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-PwfSL23QkF"
      },
      "source": [
        "<small> <b> Fig. 6. Diferentes modos de operação NIRS: transiluminância, refletância e refletância diferencial (adaptado de [R1])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPyNo_qQ7n4l"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_Tissue_Absortion.png?raw=true\" width=800, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpn2nRZA766R"
      },
      "source": [
        "<small> <b> Fig. 7. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubbw0oK98B8t"
      },
      "source": [
        "<img src=\"https://github.com/Rogerio-mack/fNIRS/blob/main/images/Fig_Machine_Learning.png?raw=true\" width=800, align=\"center\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6MVK1nA8B8u"
      },
      "source": [
        "<small> <b> Fig. 8. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMmFm5hMLoLy"
      },
      "source": [
        "## Processamento dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR-fEELU7bwW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0zFa0MAL3A8"
      },
      "source": [
        "## Aprendizado de Máquina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9PI1QBKL7RK"
      },
      "source": [
        "## Aprendizado Profundo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4WoYmQpMAqa"
      },
      "source": [
        "## Conclusões"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wayAfhz8oSTp"
      },
      "source": [
        "## Referências\n",
        "\n",
        "### Revisão\n",
        "\n",
        "1. R. K. Almajidy, K. Mankodiya, M. Abtahi and U. G. Hofmann, \"A Newcomer's Guide to Functional Near Infrared Spectroscopy Experiments,\" in IEEE Reviews in Biomedical Engineering, vol. 13, pp. 292-308, 2020, doi: 10.1109/RBME.2019.2944351.\n",
        "\n",
        "1. Scholkmann, F., Kleiser, S., Metz, A. J., Zimmermann, R., Mata Pavia, J., Wolf, U., & Wolf, M. (2014). A review on continuous wave functional near-infrared spectroscopy and imaging instrumentation and methodology. NeuroImage, 85, 6–27. doi:10.1016/j.neuroimage.2013.05.004 \n",
        "\n",
        "1. Strangman, G., Boas, D. A., & Sutton, J. P. (2002). Non-invasive neuroimaging using near-infrared light. Biological Psychiatry, 52(7), 679–693. doi:10.1016/s0006-3223(02)01550-0 \n",
        "\n",
        "1. Glover G. H. (2011). Overview of functional magnetic resonance imaging. Neurosurgery clinics of North America, 22(2), 133–vii. https://doi.org/10.1016/j.nec.2010.11.001\n",
        "\n",
        "1. Kocsis L, Herman P, Eke A. The modified Beer-Lambert law revisited. Phys Med Biol. 2006 Mar 7;51(5):N91-8. doi: 10.1088/0031-9155/51/5/N02. Epub 2006 Feb 15. PMID: 16481677.\n",
        "\n",
        "1. S. Lloyd-Fox, A. Blasi, C.E. Elwell, Illuminating the developing brain: The past, present and future of functional near infrared spectroscopy,\n",
        "Neuroscience & Biobehavioral Reviews, Volume 34, Issue 3, 2010, Pages 269-284, ISSN 0149-7634, https://doi.org/10.1016/j.neubiorev.2009.07.008.\n",
        "\n",
        "## Hardware\n",
        "\n",
        "1. Francis Tsow, Anupam Kumar, SM Hadi Hosseini, Audrey Bowden,\n",
        "A low-cost, wearable, do-it-yourself functional near-infrared spectroscopy (DIY-fNIRS) headband, HardwareX, Volume 10, 2021, e00204, ISSN 2468-0672,\n",
        "https://doi.org/10.1016/j.ohx.2021.e00204. \n",
        "\n",
        "## Aplicações\n",
        "\n",
        "1. Bakker, A., Smith, B., Ainslie, P., & Smith, K. (2012). Near-Infrared Spectroscopy. Applied Aspects of Ultrasonography in Humans. doi:10.5772/32493 \n",
        "\n",
        "1. Berkman, E. T., & Falk, E. B. (2013). Beyond Brain Mapping. Current Directions in Psychological Science, 22(1), 45–50. doi:10.1177/0963721412469394 \n",
        "\n",
        "1. Di Domenico, S. I., Rodrigo, A. H., Dong, M., Fournier, M. A., Ayaz, H., Ryan, R. M., & Ruocco, A. C. Functional Near-Infrared Spectroscopy:\n",
        "Proof of Concept for Its Application in Social Neuroscience (2019). Neuroergonomics, 169–173. doi:10.1016/b978-0-12-811926-6.00028-2 \n",
        "\n",
        "1. Irani, F., Platek, S. M., Bunce, S., Ruocco, A. C., & Chute, D. (2007). Functional Near Infrared Spectroscopy (fNIRS): An Emerging Neuroimaging Technology with Important Applications for the Study of Brain Disorders. The Clinical Neuropsychologist, 21(1), 9–37. doi:10.1080/13854040600910018 \n",
        "\n",
        "## Interface Homem-Máquina\n",
        "\n",
        "1. Noman Naseer 1 and Keum-Shik Hong, fNIRS-based brain-computer interfaces: a review, Front. Hum. Neurosci., 28 January 2015, https://doi.org/10.3389/fnhum.2015.00003. \n",
        "\n",
        "1. Moses, D.A., Leonard, M.K., Makin, J.G. et al. Real-time decoding of question-and-answer speech dialogue using human cortical activity. Nat Commun 10, 3096 (2019). https://doi.org/10.1038/s41467-019-10994-4\n",
        "\n",
        "1. FaceBook. Request for proposals on engineering approaches to responsible neural interface design. Disponível em: https://research.fb.com/programs/research-awards/proposals/request-for-proposals-on-engineering-approaches-to-responsible-neural-interface-design/. Acesso: 16/07/2021.\n",
        "\n",
        "1. FaceBook. BCI milestone: New research from UCSF with support from Facebook shows the potential of brain-computer interfaces for restoring speech communication. Disponível em: https://tech.fb.com/bci-milestone-new-research-from-ucsf-with-support-from-facebook-shows-the-potential-of-brain-computer-interfaces-for-restoring-speech-communication/. Acesso: 16/07/2021.\n",
        "\n",
        "## Tratamento e Análise dos Sinais\n",
        "\n",
        "1. Dans, P.W.; Foglia, S.D.; Nelson, A.J. Data Processing in\n",
        "Functional Near-Infrared Spectroscopy (fNIRS) Motor Control\n",
        "Research. Brain Sci. 2021, 11, 606. https://doi.org/10.3390/ \n",
        "\n",
        "1. Herold F, Wiegel P, Scholkmann F, Müller NG. Applications of Functional Near-Infrared Spectroscopy (fNIRS) Neuroimaging in Exercise–Cognition Science: A Systematic, Methodology-Focused Review. Journal of Clinical Medicine. 2018; 7(12):466. https://doi.org/10.3390/jcm7120466\n",
        "\n",
        "1. Worsley, K. J., Liao, C. H., Aston, J., Petre, V., Duncan, G. H., Morales, F., & Evans, A. C. (2002). A General Statistical Analysis for fMRI Data. NeuroImage, 15(1), 1–15. doi:10.1006/nimg.2001.0933  \n",
        "\n",
        "1. Takahiro ImaiTakanori SatoIsao NambuYasuhiro Wada, Estimating Brain Activity of Motor Learning by Using fNIRS-GLM Analysis, Neural Information Processing, 2012, Springer Berlin Heidelberg, 401--408, 10.1007/978-3-642-34475-6_48 \n",
        "\n",
        "1. Meryem A. Yücel, Alexander v. Lühmann, Felix Scholkmann, Judit Gervain, Ippeita Dan, Hasan Ayaz, David Boas, Robert J. Cooper, Joseph Culver, Clare E. Elwell, Adam Eggebrecht, Maria A. Franceschini, Christophe Grova, Fumitaka Homae, Frédéric Lesage, Hellmuth Obrig, Ilias Tachtsidis, Sungho Tak, Yunjie Tong, Alessandro Torricelli, Heidrun Wabnitz, Martin Wolf, \"Best practices for fNIRS publications,\" Neurophoton. 8(1) 012101 (7 January 2021) https://doi.org/10.1117/1.NPh.8.1.012101\n",
        "\n",
        "1. von Lühmann A, Ortega-Martinez A, Boas DA and Yücel MA (2020) Using the General Linear Model to Improve Performance in fNIRS Single Trial Analysis and Classification: A Perspective. Front. Hum. Neurosci. 14:30. doi: 10.3389/fnhum.2020.00030\n",
        "\n",
        "1. Worsley KJ, Liao CH, Aston J, Petre V, Duncan GH, Morales F, Evans AC. A general statistical analysis for fMRI data. Neuroimage. 2002 Jan;15(1):1-15. doi: 10.1006/nimg.2001.0933. PMID: 11771969.\n",
        "\n",
        "1. Friston KJ, Holmes AP, Poline JB, Grasby PJ, Williams SC, Frackowiak RS, Turner R. Analysis of fMRI time-series revisited. Neuroimage. 1995 Mar;2(1):45-53. doi: 10.1006/nimg.1995.1007. PMID: 9343589.\n",
        "\n",
        "## Aprendizado de Máquina e Aprendizado Profundo\n",
        "\n",
        "1. Fernandez Rojas, R., Huang, X. & Ou, KL. A Machine Learning Approach for the Identification of a Biomarker of Human Pain using fNIRS. Sci Rep 9, 5645 (2019). https://doi.org/10.1038/s41598-019-42098-w\n",
        "\n",
        "1. Gao, Yuanyuan & Chao, Hanqing & Cavuoto, Lora & Yan, Pingkun & Kruger, Uwe & Norfleet, Jack & Makled, Basiel & Schwaitzberg, Steven & De, Suvranu & Intes, Xavier. (2020). Deep Learning-based Motion Artifact Removal in Functional Near-Infrared Spectroscopy (fNIRS). 10.13140/RG.2.2.33766.86089. \n",
        "\n",
        "1. Pinti Paola, Scholkmann Felix, Hamilton Antonia, Burgess Paul, Tachtsidis Ilias, Current Status and Issues Regarding Pre-processing of fNIRS Neuroimaging Data: An Investigation of Diverse Signal Filtering Methods Within a General Linear Model Framework, Frontiers in Human Neuroscience, Volume 12, 2019, page 505, DOI=10.3389/fnhum.2018.00505    \n",
        "\t\n",
        "1. Aras, Roque & Abtahi, Mohammadreza & Mankodiya, Kunal. (2019). An end-to-end (deep) neural network applied to raw EEG, fNIRs and body motion data for data fusion and BCI classification task without any pre-/post-processing. \n",
        "\n",
        "1. Ma T, Chen W, Li X, Xia Y, Zhu X, He S. fNIRS Signal Classification Based on Deep Learning in Rock-Paper-Scissors Imagery Task. Applied Sciences. 2021; 11(11):4922. https://doi.org/10.3390/app11114922"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN06W4GOLOHy",
        "outputId": "d5d71d12-e4c7-48da-90b2-3871cf5db499"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install mne-nirs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mne-nirs\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6f8cc5facf5cd5011563671244ff0424cabde9d58d0e9b1f1c4262bf694a/mne-nirs-0.0.6.tar.gz\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from mne-nirs) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mne-nirs) (1.4.1)\n",
            "Collecting mne>=0.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f7/2bf5de3fad42b66d00ee27539bc3be0260b4e66fdecc12f740cdf2daf2e7/mne-0.23.0-py3-none-any.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 12.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mne-nirs\n",
            "  Building wheel for mne-nirs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mne-nirs: filename=mne_nirs-0.0.6-cp37-none-any.whl size=54360 sha256=c162897a780ffb978260dac328c8ebc00d3ca5ad2b71922606d3e3285d73ff91\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/cf/a8/22a0ae5a59f13671a8313dc7c0ffd91eb9c671c372c9eba1ef\n",
            "Successfully built mne-nirs\n",
            "Installing collected packages: mne, mne-nirs\n",
            "Successfully installed mne-0.23.0 mne-nirs-0.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVhnhAU3NFh5"
      },
      "source": [
        "\n",
        "\n",
        "# Preprocessing functional near-infrared spectroscopy (fNIRS) data\n",
        "\n",
        "This tutorial covers how to convert functional near-infrared spectroscopy\n",
        "(fNIRS) data from raw measurements to relative oxyhaemoglobin (HbO) and\n",
        "deoxyhaemoglobin (HbR) concentration.\n",
        "\n",
        "Here we will work with the `fNIRS motor data <fnirs-motor-dataset>`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545,
          "referenced_widgets": [
            "99b165564b3c4bbf9571786a1c617de2",
            "9f39e6ec12bd46fc89fbef50c456fa73",
            "e2f97557cec942cda6265d28ad06c1be",
            "903a22f9a66a496bb28a9a4c09ae7a4e",
            "5c34233762fe47d88cf2bb82c2806b68",
            "007f2a4161ac43818715098745f0c756",
            "4fda4eb1c8a34c0db7c883f1c765e112",
            "baf25102ac654f6284f1f7bc73e9791e"
          ]
        },
        "id": "YIkAjzn1NFh6",
        "outputId": "15b10d6f-c821-4412-e03d-0296e030666e"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import compress\n",
        "\n",
        "import mne\n",
        "\n",
        "\n",
        "fnirs_data_folder = mne.datasets.fnirs_motor.data_path()\n",
        "fnirs_cw_amplitude_dir = os.path.join(fnirs_data_folder, 'Participant-1')\n",
        "raw_intensity = mne.io.read_raw_nirx(fnirs_cw_amplitude_dir, verbose=True)\n",
        "raw_intensity.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using default location ~/mne_data for fnirs_motor...\n",
            "Creating ~/mne_data\n",
            "Downloading archive MNE-fNIRS-motor-data.tgz to /root/mne_data\n",
            "Downloading https://files.osf.io/v1/resources/rxvq7/providers/osfstorage/5dbf84a9cfc96c000ec957eb?version=1&action=download&direct (17.1 MB)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99b165564b3c4bbf9571786a1c617de2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=17881709.0, style=ProgressStyle(descrip…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Verifying hash c4935d19ddab35422a69f3326a01fef8.\n",
            "Decompressing the archive: /root/mne_data/MNE-fNIRS-motor-data.tgz\n",
            "(please be patient, this can take some time)\n",
            "Successfully extracted to: ['/root/mne_data/MNE-fNIRS-motor-data']\n",
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Loading /root/mne_data/MNE-fNIRS-motor-data/Participant-1\n",
            "Reading 0 ... 23238  =      0.000 ...  2974.464 secs...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "\n",
              "<table class=\"table table-hover\">\n",
              "    <tr>\n",
              "        <th>Measurement date</th>\n",
              "        <td>November 02, 2019  13:16:16 GMT</td>\n",
              "        \n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Experimenter</th>\n",
              "<td>Unknown</td>\n",
              "    </tr>\n",
              "        <th>Participant</th>\n",
              "        \n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Digitized points</th>\n",
              "        <td>31 points</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Good channels</th>\n",
              "        <td>0 magnetometer, 0 gradiometer,\n",
              "            and 0 EEG channels</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Bad channels</th>\n",
              "        <td></td>\n",
              "        \n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>EOG channels</th>\n",
              "        <td>Not available</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>ECG channels</th>\n",
              "        <td>Not available</td>\n",
              "    <tr>\n",
              "        <th>Sampling frequency</th>\n",
              "        <td>7.81 Hz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Highpass</th>\n",
              "        <td>0.00 Hz</td>\n",
              "    </tr>\n",
              "     <tr>\n",
              "        <th>Lowpass</th>\n",
              "        <td>3.91 Hz</td>\n",
              "    </tr>\n",
              "\n",
              "    <tr>\n",
              "        <th>Filenames</th>\n",
              "        <td>Participant-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Duration</th>\n",
              "        <td>00:49:34 (HH:MM:SS)</td>\n",
              "    </tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<RawNIRX | Participant-1, 56 x 23239 (2974.5 s), ~10.0 MB, data loaded>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WoWKwz4NFh7"
      },
      "source": [
        "## View location of sensors over brain surface\n",
        "\n",
        "Here we validate that the location of sources-detector pairs and channels\n",
        "are in the expected locations. Source-detector pairs are shown as lines\n",
        "between the optodes, channels (the mid point of source-detector pairs) are\n",
        "optionally shown as orange dots. Source are optionally shown as red dots and\n",
        "detectors as black.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MukbWOM3Nqow",
        "outputId": "00706eb7-d096-424c-87f6-49b11e987bf3"
      },
      "source": [
        "!pip install mayavi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mayavi\n",
            "  Using cached https://files.pythonhosted.org/packages/e9/0c/4443d5690cebaa5f20ce9d092627932c1ef38c20c925b26d153615f56b6f/mayavi-4.7.3.tar.gz\n",
            "Requirement already satisfied: apptools in /usr/local/lib/python3.7/dist-packages (from mayavi) (5.1.0)\n",
            "Requirement already satisfied: envisage in /usr/local/lib/python3.7/dist-packages (from mayavi) (6.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mayavi) (1.19.5)\n",
            "Requirement already satisfied: pyface>=6.1.1 in /usr/local/lib/python3.7/dist-packages (from mayavi) (7.3.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from mayavi) (2.6.1)\n",
            "Requirement already satisfied: traits>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from mayavi) (6.2.0)\n",
            "Requirement already satisfied: traitsui>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from mayavi) (7.2.1)\n",
            "Requirement already satisfied: vtk in /usr/local/lib/python3.7/dist-packages (from mayavi) (9.0.3)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.7/dist-packages (from apptools->mayavi) (5.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from envisage->mayavi) (57.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from pyface>=6.1.1->mayavi) (4.6.0)\n",
            "Requirement already satisfied: importlib-resources>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pyface>=6.1.1->mayavi) (5.2.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from vtk->mayavi) (3.2.2)\n",
            "Requirement already satisfied: Twisted>=17.5.0 in /usr/local/lib/python3.7/dist-packages (from vtk->mayavi) (21.2.0)\n",
            "Requirement already satisfied: autobahn>=17.7.1 in /usr/local/lib/python3.7/dist-packages (from vtk->mayavi) (21.3.1)\n",
            "Requirement already satisfied: wslink>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from vtk->mayavi) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from configobj->apptools->mayavi) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pyface>=6.1.1->mayavi) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pyface>=6.1.1->mayavi) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk->mayavi) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk->mayavi) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk->mayavi) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->vtk->mayavi) (0.10.0)\n",
            "Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk->mayavi) (5.4.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk->mayavi) (15.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk->mayavi) (21.2.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk->mayavi) (20.2.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk->mayavi) (21.0.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.5.0->vtk->mayavi) (21.3.0)\n",
            "Requirement already satisfied: cryptography>=3.4.6 in /usr/local/lib/python3.7/dist-packages (from autobahn>=17.7.1->vtk->mayavi) (3.4.7)\n",
            "Requirement already satisfied: txaio>=21.2.1 in /usr/local/lib/python3.7/dist-packages (from autobahn>=17.7.1->vtk->mayavi) (21.2.1)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=17.5.0->vtk->mayavi) (2.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.4.6->autobahn>=17.7.1->vtk->mayavi) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.4.6->autobahn>=17.7.1->vtk->mayavi) (2.20)\n",
            "Building wheels for collected packages: mayavi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542,
          "referenced_widgets": [
            "cde927b920904b8ab5904b8b77c6d0bc",
            "ad81f9f2e8aa4b61a43932ccfe6afd00",
            "4348776d41c44de6b379392931474f11",
            "696d80e0592e4626a0df418fccb5b059",
            "05b235d067ac448a86a39cc8e86bb8e1",
            "0c3baec7e8714ee9891adf613536b345",
            "aeb1b0b59831423d89489a346a19ebc9",
            "a546faeafae94e5ab807f7c94b8c4bd7"
          ]
        },
        "id": "saSxJ4QLNFh7",
        "outputId": "2dca2878-5277-430b-99ed-7742eec867fd"
      },
      "source": [
        "subjects_dir = mne.datasets.sample.data_path() + '/subjects'\n",
        "\n",
        "fig = mne.viz.create_3d_figure(size=(800, 600), bgcolor='white')\n",
        "fig = mne.viz.plot_alignment(raw_intensity.info, show_axes=True,\n",
        "                             subject='fsaverage', coord_frame='mri',\n",
        "                             trans='fsaverage', surfaces=['brain'],\n",
        "                             fnirs=['channels', 'pairs',\n",
        "                                    'sources', 'detectors'],\n",
        "                             subjects_dir=subjects_dir, fig=fig)\n",
        "mne.viz.set_3d_view(figure=fig, azimuth=20, elevation=60, distance=0.4,\n",
        "                    focalpoint=(0., -0.01, 0.02))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using default location ~/mne_data for sample...\n",
            "Downloading archive MNE-sample-data-processed.tar.gz to /root/mne_data\n",
            "Downloading https://files.osf.io/v1/resources/rxvq7/providers/osfstorage/59c0e26f9ad5a1025c4ab159?version=5&action=download&direct (1.54 GB)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cde927b920904b8ab5904b8b77c6d0bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1652769680.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Verifying hash 12b75d1cb7df9dfb4ad73ed82f61094f.\n",
            "Decompressing the archive: /root/mne_data/MNE-sample-data-processed.tar.gz\n",
            "(please be patient, this can take some time)\n",
            "Successfully extracted to: ['/root/mne_data/MNE-sample-data']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d648eeea22b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msubjects_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/subjects'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_3d_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbgcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m fig = mne.viz.plot_alignment(raw_intensity.info, show_axes=True,\n\u001b[1;32m      5\u001b[0m                              \u001b[0msubject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fsaverage'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mri'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mne/viz/backends/renderer.py\u001b[0m in \u001b[0;36mcreate_3d_figure\u001b[0;34m(size, bgcolor, smooth_shading, handle, scene)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mbgcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbgcolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msmooth_shading\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmooth_shading\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mne/viz/backends/renderer.py\u001b[0m in \u001b[0;36m_get_renderer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mset_3d_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_3d_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mne/viz/backends/renderer.py\u001b[0m in \u001b[0;36m_get_3d_backend\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 raise RuntimeError(f'Could not load any valid 3D backend: '\n\u001b[0m\u001b[1;32m    154\u001b[0m                                    f'{\", \".join(VALID_3D_BACKENDS)}')\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Could not load any valid 3D backend: pyvista, mayavi, notebook"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cER1OZbqNFh8"
      },
      "source": [
        "## Selecting channels appropriate for detecting neural responses\n",
        "\n",
        "First we remove channels that are too close together (short channels) to\n",
        "detect a neural response (less than 1 cm distance between optodes).\n",
        "These short channels can be seen in the figure above.\n",
        "To achieve this we pick all the channels that are not considered to be short.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0DdnVRSNFh8"
      },
      "source": [
        "picks = mne.pick_types(raw_intensity.info, meg=False, fnirs=True)\n",
        "dists = mne.preprocessing.nirs.source_detector_distances(\n",
        "    raw_intensity.info, picks=picks)\n",
        "raw_intensity.pick(picks[dists > 0.01])\n",
        "raw_intensity.plot(n_channels=len(raw_intensity.ch_names),\n",
        "                   duration=500, show_scrollbars=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga0dSNrMNFh9"
      },
      "source": [
        "## Converting from raw intensity to optical density\n",
        "\n",
        "The raw intensity values are then converted to optical density.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWISMz7ZNFh9"
      },
      "source": [
        "raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
        "raw_od.plot(n_channels=len(raw_od.ch_names),\n",
        "            duration=500, show_scrollbars=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb3U3M2CNFh-"
      },
      "source": [
        "## Evaluating the quality of the data\n",
        "\n",
        "At this stage we can quantify the quality of the coupling\n",
        "between the scalp and the optodes using the scalp coupling index. This\n",
        "method looks for the presence of a prominent synchronous signal in the\n",
        "frequency range of cardiac signals across both photodetected signals.\n",
        "\n",
        "In this example the data is clean and the coupling is good for all\n",
        "channels, so we will not mark any channels as bad based on the scalp\n",
        "coupling index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IPH5oOlNFh-"
      },
      "source": [
        "sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(sci)\n",
        "ax.set(xlabel='Scalp Coupling Index', ylabel='Count', xlim=[0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8MpJJfjNFh_"
      },
      "source": [
        "In this example we will mark all channels with a SCI less than 0.5 as bad\n",
        "(this dataset is quite clean, so no channels are marked as bad).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3erpjoWNFh_"
      },
      "source": [
        "raw_od.info['bads'] = list(compress(raw_od.ch_names, sci < 0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mprTCmZMNFh_"
      },
      "source": [
        "At this stage it is appropriate to inspect your data\n",
        "(for instructions on how to use the interactive data visualisation tool\n",
        "see `tut-visualize-raw`)\n",
        "to ensure that channels with poor scalp coupling have been removed.\n",
        "If your data contains lots of artifacts you may decide to apply\n",
        "artifact reduction techniques as described in `ex-fnirs-artifacts`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yd7ht2_NFiA"
      },
      "source": [
        "## Converting from optical density to haemoglobin\n",
        "\n",
        "Next we convert the optical density data to haemoglobin concentration using\n",
        "the modified Beer-Lambert law.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVwuTDwqNFiA"
      },
      "source": [
        "raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od)\n",
        "raw_haemo.plot(n_channels=len(raw_haemo.ch_names),\n",
        "               duration=500, show_scrollbars=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUBvhusLNFiA"
      },
      "source": [
        "## Removing heart rate from signal\n",
        "\n",
        "The haemodynamic response has frequency content predominantly below 0.5 Hz.\n",
        "An increase in activity around 1 Hz can be seen in the data that is due to\n",
        "the person's heart beat and is unwanted. So we use a low pass filter to\n",
        "remove this. A high pass filter is also included to remove slow drifts\n",
        "in the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l8kR7LsNFiA"
      },
      "source": [
        "fig = raw_haemo.plot_psd(average=True)\n",
        "fig.suptitle('Before filtering', weight='bold', size='x-large')\n",
        "fig.subplots_adjust(top=0.88)\n",
        "raw_haemo = raw_haemo.filter(0.05, 0.7, h_trans_bandwidth=0.2,\n",
        "                             l_trans_bandwidth=0.02)\n",
        "fig = raw_haemo.plot_psd(average=True)\n",
        "fig.suptitle('After filtering', weight='bold', size='x-large')\n",
        "fig.subplots_adjust(top=0.88)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BvmuBGpNFiB"
      },
      "source": [
        "## Extract epochs\n",
        "\n",
        "Now that the signal has been converted to relative haemoglobin concentration,\n",
        "and the unwanted heart rate component has been removed, we can extract epochs\n",
        "related to each of the experimental conditions.\n",
        "\n",
        "First we extract the events of interest and visualise them to ensure they are\n",
        "correct.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY_8jTkTNFiB"
      },
      "source": [
        "events, _ = mne.events_from_annotations(raw_haemo, event_id={'1.0': 1,\n",
        "                                                             '2.0': 2,\n",
        "                                                             '3.0': 3})\n",
        "event_dict = {'Control': 1, 'Tapping/Left': 2, 'Tapping/Right': 3}\n",
        "fig = mne.viz.plot_events(events, event_id=event_dict,\n",
        "                          sfreq=raw_haemo.info['sfreq'])\n",
        "fig.subplots_adjust(right=0.7)  # make room for the legend"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVylIXwGNFiB"
      },
      "source": [
        "Next we define the range of our epochs, the rejection criteria,\n",
        "baseline correction, and extract the epochs. We visualise the log of which\n",
        "epochs were dropped.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkePpU8bNFiB"
      },
      "source": [
        "reject_criteria = dict(hbo=80e-6)\n",
        "tmin, tmax = -5, 15\n",
        "\n",
        "epochs = mne.Epochs(raw_haemo, events, event_id=event_dict,\n",
        "                    tmin=tmin, tmax=tmax,\n",
        "                    reject=reject_criteria, reject_by_annotation=True,\n",
        "                    proj=True, baseline=(None, 0), preload=True,\n",
        "                    detrend=None, verbose=True)\n",
        "epochs.plot_drop_log()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Culc1ccGNFiC"
      },
      "source": [
        "## View consistency of responses across trials\n",
        "\n",
        "Now we can view the haemodynamic response for our tapping condition.\n",
        "We visualise the response for both the oxy- and deoxyhaemoglobin, and\n",
        "observe the expected peak in HbO at around 6 seconds consistently across\n",
        "trials, and the consistent dip in HbR that is slightly delayed relative to\n",
        "the HbO peak.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV2Na6EbNFiC"
      },
      "source": [
        "epochs['Tapping'].plot_image(combine='mean', vmin=-30, vmax=30,\n",
        "                             ts_args=dict(ylim=dict(hbo=[-15, 15],\n",
        "                                                    hbr=[-15, 15])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCO6ieSnNFiC"
      },
      "source": [
        "We can also view the epoched data for the control condition and observe\n",
        "that it does not show the expected morphology.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kkcYGnqNFiC"
      },
      "source": [
        "epochs['Control'].plot_image(combine='mean', vmin=-30, vmax=30,\n",
        "                             ts_args=dict(ylim=dict(hbo=[-15, 15],\n",
        "                                                    hbr=[-15, 15])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s1D1RywNFiC"
      },
      "source": [
        "## View consistency of responses across channels\n",
        "\n",
        "Similarly we can view how consistent the response is across the optode\n",
        "pairs that we selected. All the channels in this data are located over the\n",
        "motor cortex, and all channels show a similar pattern in the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVWim3GhNFiD"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 6))\n",
        "clims = dict(hbo=[-20, 20], hbr=[-20, 20])\n",
        "epochs['Control'].average().plot_image(axes=axes[:, 0], clim=clims)\n",
        "epochs['Tapping'].average().plot_image(axes=axes[:, 1], clim=clims)\n",
        "for column, condition in enumerate(['Control', 'Tapping']):\n",
        "    for ax in axes[:, column]:\n",
        "        ax.set_title('{}: {}'.format(condition, ax.get_title()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl6uP4yuNFiD"
      },
      "source": [
        "## Plot standard fNIRS response image\n",
        "\n",
        "Next we generate the most common visualisation of fNIRS data: plotting\n",
        "both the HbO and HbR on the same figure to illustrate the relation between\n",
        "the two signals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYZOQeGPNFiD"
      },
      "source": [
        "evoked_dict = {'Tapping/HbO': epochs['Tapping'].average(picks='hbo'),\n",
        "               'Tapping/HbR': epochs['Tapping'].average(picks='hbr'),\n",
        "               'Control/HbO': epochs['Control'].average(picks='hbo'),\n",
        "               'Control/HbR': epochs['Control'].average(picks='hbr')}\n",
        "\n",
        "# Rename channels until the encoding of frequency in ch_name is fixed\n",
        "for condition in evoked_dict:\n",
        "    evoked_dict[condition].rename_channels(lambda x: x[:-4])\n",
        "\n",
        "color_dict = dict(HbO='#AA3377', HbR='b')\n",
        "styles_dict = dict(Control=dict(linestyle='dashed'))\n",
        "\n",
        "mne.viz.plot_compare_evokeds(evoked_dict, combine=\"mean\", ci=0.95,\n",
        "                             colors=color_dict, styles=styles_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHcHUT2LNFiD"
      },
      "source": [
        "## View topographic representation of activity\n",
        "\n",
        "Next we view how the topographic activity changes throughout the response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5LpmbfYNFiD"
      },
      "source": [
        "times = np.arange(-3.5, 13.2, 3.0)\n",
        "topomap_args = dict(extrapolate='local')\n",
        "epochs['Tapping'].average(picks='hbo').plot_joint(\n",
        "    times=times, topomap_args=topomap_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmxc1gn5NFiD"
      },
      "source": [
        "## Compare tapping of left and right hands\n",
        "\n",
        "Finally we generate topo maps for the left and right conditions to view\n",
        "the location of activity. First we visualise the HbO activity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttQfJy5dNFiE"
      },
      "source": [
        "times = np.arange(4.0, 11.0, 1.0)\n",
        "epochs['Tapping/Left'].average(picks='hbo').plot_topomap(\n",
        "    times=times, **topomap_args)\n",
        "epochs['Tapping/Right'].average(picks='hbo').plot_topomap(\n",
        "    times=times, **topomap_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrF998hNNFiE"
      },
      "source": [
        "And we also view the HbR activity for the two conditions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTaYfraaNFiE"
      },
      "source": [
        "epochs['Tapping/Left'].average(picks='hbr').plot_topomap(\n",
        "    times=times, **topomap_args)\n",
        "epochs['Tapping/Right'].average(picks='hbr').plot_topomap(\n",
        "    times=times, **topomap_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ5w5srcNFiE"
      },
      "source": [
        "And we can plot the comparison at a single time point for two conditions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSlnIJ40NFiE"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(9, 5),\n",
        "                         gridspec_kw=dict(width_ratios=[1, 1, 1, 0.1]))\n",
        "vmin, vmax, ts = -8, 8, 9.0\n",
        "\n",
        "evoked_left = epochs['Tapping/Left'].average()\n",
        "evoked_right = epochs['Tapping/Right'].average()\n",
        "\n",
        "evoked_left.plot_topomap(ch_type='hbo', times=ts, axes=axes[0, 0],\n",
        "                         vmin=vmin, vmax=vmax, colorbar=False,\n",
        "                         **topomap_args)\n",
        "evoked_left.plot_topomap(ch_type='hbr', times=ts, axes=axes[1, 0],\n",
        "                         vmin=vmin, vmax=vmax, colorbar=False,\n",
        "                         **topomap_args)\n",
        "evoked_right.plot_topomap(ch_type='hbo', times=ts, axes=axes[0, 1],\n",
        "                          vmin=vmin, vmax=vmax, colorbar=False,\n",
        "                          **topomap_args)\n",
        "evoked_right.plot_topomap(ch_type='hbr', times=ts, axes=axes[1, 1],\n",
        "                          vmin=vmin, vmax=vmax, colorbar=False,\n",
        "                          **topomap_args)\n",
        "\n",
        "evoked_diff = mne.combine_evoked([evoked_left, evoked_right], weights=[1, -1])\n",
        "\n",
        "evoked_diff.plot_topomap(ch_type='hbo', times=ts, axes=axes[0, 2:],\n",
        "                         vmin=vmin, vmax=vmax, colorbar=True,\n",
        "                         **topomap_args)\n",
        "evoked_diff.plot_topomap(ch_type='hbr', times=ts, axes=axes[1, 2:],\n",
        "                         vmin=vmin, vmax=vmax, colorbar=True,\n",
        "                         **topomap_args)\n",
        "\n",
        "for column, condition in enumerate(\n",
        "        ['Tapping Left', 'Tapping Right', 'Left-Right']):\n",
        "    for row, chroma in enumerate(['HbO', 'HbR']):\n",
        "        axes[row, column].set_title('{}: {}'.format(chroma, condition))\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLHaI8IpNFiF"
      },
      "source": [
        "Lastly, we can also look at the individual waveforms to see what is\n",
        "driving the topographic plot above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRX194gnNFiF"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
        "mne.viz.plot_evoked_topo(epochs['Left'].average(picks='hbo'), color='b',\n",
        "                         axes=axes, legend=False)\n",
        "mne.viz.plot_evoked_topo(epochs['Right'].average(picks='hbo'), color='r',\n",
        "                         axes=axes, legend=False)\n",
        "\n",
        "# Tidy the legend.\n",
        "leg_lines = [line for line in axes.lines if line.get_c() == 'b'][:1]\n",
        "leg_lines.append([line for line in axes.lines if line.get_c() == 'r'][0])\n",
        "fig.legend(leg_lines, ['Left', 'Right'], loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}